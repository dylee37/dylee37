{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10cb443",
   "metadata": {},
   "source": [
    "# ⭐AI 강의 II - WRAP UP\n",
    "\n",
    "## AI를 위한 python과 math\n",
    "### AI를 위한 준비과정\n",
    "1. 파이썬 기본 문법\n",
    "2. 클래스\n",
    "3. Numpy\n",
    "4. Pandas\n",
    "5. Matplot\n",
    "6. Matplot\n",
    "7. 선형회귀\n",
    "8. 로지스틱회귀\n",
    "9. Gradient Decent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d2c2b1",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "EDA를 통해 데이터를 잘 이해한다.\n",
    "- EDA: 데이터를 살펴보는 행동\n",
    "- 데이터 샘플을 확인\n",
    "- 필드 확인\n",
    "- 기본 통계량 확인\n",
    "- 분포 확인\n",
    "- 상관계수 확인\n",
    "- 등등\n",
    "\n",
    "![img](./img/wrap1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a89e6",
   "metadata": {},
   "source": [
    "## MLP (다층 퍼셉트론)\n",
    "\n",
    "MLP(Multi-Layer Perceptron)\n",
    "- 하나 이상의 은닉층을 가진 신경망\n",
    "- 활성화 함수와 함께 복잡한 패턴을 학습하고 분류\n",
    "- 딥러닝 학습의 시작점\n",
    "\n",
    "![img](./img/wrap2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d03b54",
   "metadata": {},
   "source": [
    "## 토큰화 / 임베딩\n",
    "\n",
    "토큰화\n",
    "- 문장을 토큰 단위로 나누는 과정\n",
    "\n",
    "임베딩\n",
    "- 각 토큰화된 단어들에게 의미를 부여하는 과정\n",
    "- 의미공간의 vector 값을 갖게 된다.\n",
    "\n",
    "![img](./img/wrap3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f756a",
   "metadata": {},
   "source": [
    "## 합성 데이터와 데이터 증강\n",
    "\n",
    "합성 데이터 (synthetic data)\n",
    "- 실제 세계에 없는 새로운 데이터를 생성하는 것\n",
    "\n",
    "데이터 증강(data augmentation)\n",
    "- 실제 데이터를 변형하여 데이터셋을 확보하는 것\n",
    "\n",
    "![img](./img/wrap4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7001d27",
   "metadata": {},
   "source": [
    "## CNN (Convolutional Neural Network)\n",
    "- 이미지, 영상에서 시각 데이터의 특징을 기반으로 사물을 인식할 수 있는 딥러닝 알고리즘\n",
    "- 2012년  AI 겨울을 끝내고, CNN 기반의 AlexNet을 통해 AI 시대를 다시 열게 됨\n",
    "\n",
    "![img](./img/wrap5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d91258",
   "metadata": {},
   "source": [
    "## RNN / LSTM\n",
    "\n",
    "### RNN (Recurrent Neural Network)\n",
    "- AI에서 핵심 딥러닝 알고리즘 중 하나\n",
    "- 시퀀스 데이터를 학습시키고, 추론시키는 데에 특화된 딥러닝\n",
    "- Attention의 필요성을 이해하기 위해 학습\n",
    "\n",
    "![img](./img/wrap6.png)\n",
    "\n",
    "### LSTM (Long Short-Term Memory)\n",
    "- RNN의 가장 큰 단점: 과거 정보를 오래 기억하지 못한다.\n",
    "  - 과거 정보가 사라지는 이유: 이전 정보를 계속 곱하면서 진행하기 때문 (기울기 소실이 발생)\n",
    "- LSTM은 입력/출력/망각 게이트를 통해 중요한 정보는 기억하고 필요없는 정보는 버림\n",
    "  - 긴 문맥도 이해할 수 있게 됨\n",
    "- LSTM의 한계점: 문맥 내 모든 단어 간의 관계를 한 번에 파악하기 어려움\n",
    "\n",
    "![img](./img/wrap7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adac470",
   "metadata": {},
   "source": [
    "## Attention과 Trasformer 모델\n",
    "### Attention\n",
    "- 입력 데이터에 중요한 부분에 집중할 수 있게 돕는 기술\n",
    "- 문맥을 이해하는 벡터를 만드는 역할을 함\n",
    "\n",
    "### Transformer\n",
    "- 어텐션을 인코더, 디코더 형태로 활용하여 Foundation Model 시대를 열게된 딥러닝 모델\n",
    "\n",
    "![img](./img/wrap8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8010c",
   "metadata": {},
   "source": [
    "## Transformer 기반 이미지 모델\n",
    "- 트랜스포머는 text, 이미지, 음성 등 여러 모달을 다루는 모델의 성능을 극대화시킴\n",
    "\n",
    "![img](./img/wrap9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d24fd",
   "metadata": {},
   "source": [
    "## RAG (Retrieval-Augmented Generation)\n",
    "- LLM이 신뢰성 있는 최신 정보를 참고하여 답변하게 하는 기술\n",
    "- 환각 현상을 줄일 수 있음\n",
    "\n",
    "<LangChain 사용방법과 함께 RAG 구현 방법을 실습>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550b4e4",
   "metadata": {},
   "source": [
    "## PEFT\n",
    "### 효율적인 파인튜닝(PEFT)\n",
    "- 모든 파라미터를 변경하지 않고, 일부 파라미터만 변경하여 파인튜닝의 효율을 극대화시킴\n",
    "- LoRA를 중심적으로 다룸\n",
    "\n",
    "![img](./img/wrap10.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
