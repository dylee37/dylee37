{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c51551e",
   "metadata": {},
   "source": [
    "# ⭐ <span style=\"background-color: #D9E5FF\">Post-training (Instruction-tuning, RLHF)</span>\n",
    "\n",
    "> Contents\n",
    "1. Pre-training vs Post-training\n",
    "2. Instruction-tuning\n",
    "3. Reinforcement Learning from Human Feedback (RLHF)\n",
    "4. What's Next?\n",
    "\n",
    "> 학습 목표\n",
    "- 거대 언어모델의 학습 패러다임에 대한 이해를 한다.\n",
    "- Instruction-tuning에 대해 설명할 수 있다.\n",
    "- RLHF의 개념을 이해하고 instruction-tuning과의 차이를 이해한다.\n",
    "- RLHF 이후의 post-training 접근들을 이해한다.\n",
    "\n",
    "## 0. 학습 시작(오버뷰)\n",
    "> 거대 언어모델의 학습 패러다임이 어떻게 되는가?\n",
    "- Pre-training(사전학습)과 Post-training(사후 학습)의 개념과 차이\n",
    "> 언어모델이 어떻게 사용자의 질문에 응답을 할 수 있게 되었는가\n",
    "- Instruction-tuning에 대한 개념 이해\n",
    "> 언어모델이 어떻게 사용자가 선호하는 답변을 내도록 학습이 되었는가\n",
    "- Instruction-tuning의 한계와 RLHF의 개념에 대한 이해\n",
    "> RLHF 이후에 post-training 접근들은 어떤게 있는가\n",
    "- RLHF의 한계점과 이후 접근들에 대한 소개(DPO, RLVR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40aca4d5",
   "metadata": {},
   "source": [
    "## 1. Pre-training vs Post-training\n",
    "> 거대 언어 모델 학습 패러다임\n",
    "- 최근 거대 언어 모델을 학습하기 위해서는 크게 두 가지 단계로 나뉜다.\n",
    "  - Pre-training(사전 학습): 방대한 인터넷 텍스트를 통해 언어와 지식을 배우는 단계\n",
    "  - Post-training(사후 학습): 사람이 원하는 방식으로 대화하고, 안전하고, 유용하게 만드는 단계\n",
    "\n",
    "> Pre-training(사전 학습)\n",
    "- 방대한 인터넷 텍스트 데이터를 이용한 Self-supervised learning을 통해 언어 패턴, 지식 등을 배운다.\n",
    "- 학습 목표: 다음 단어 예측(Next Token Prediction)\n",
    "- 예시:\n",
    "  - \"내일은 비가 ___\" -> \"온다\"의 확률을 가장 높게 만들도록 파라미터 업데이트\n",
    "\n",
    "![img](./img/l1.png)\n",
    "\n",
    "> Pre-training 후 모델의 응답 예시\n",
    "- 다음 단어를 예측하는 데에 강점을 보이지만 질문에 대한 대답을 하지는 않음\n",
    "\n",
    "![img](./img/l2.png)\n",
    "\n",
    "> Post-training (사후 학습)\n",
    "- 유저의 의도를 파악하고 원하는 답변을 모델이 응답하도록 사후 학습을 진행한다.\n",
    "- 대표 기법:\n",
    "  - Instruction-tuning, RLHF(Reinforcement Learning from Human Feedback), DPO(Direct Preference Optimization), RLVR(Reinforcement Learning with Verifiable Reward)\n",
    "\n",
    "![img](./img/l3.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358b0d24",
   "metadata": {},
   "source": [
    "## 2. Instruction-tuning\n",
    "> 사전학습 후 언어모델의 한계\n",
    "- 사전학습 후 거대 언어모델은 유저의 의도와 일치하지 않음\n",
    "- 이를 해결하기 위해 파인 튜닝을 진행\n",
    "\n",
    "![image](./img/l4.png)\n",
    "\n",
    "> 파인튜닝이란?\n",
    "- 이미 사전학습된 기존 모델에 특정 작업이나 도메인에 맞게 추가로 학습시키는 과정\n",
    "\n",
    "![image](./img/l5.png)\n",
    "\n",
    "> Instruction-tuning이란?\n",
    "- 언어모델이 사림이 내린 지시문(instruction)을 따르도록 학습하는 단계\n",
    "- 정답 레이블이 요구되며, 다양한 태스크를 풀 수 있도록 적응하는 것에 중점을 둠\n",
    "\n",
    "![image](./img/l6.png)\n",
    "\n",
    "- 다양한 태스크에서 (지시문, 응답) 쌍을 모아서 언어모델을 파인튜닝한다.\n",
    "\n",
    "![image](./img/l7.png)\n",
    "\n",
    "- 새로운(모델이 학습하지 않은) 태스크에서 평가를 진행한다.\n",
    "\n",
    "![image](./img/l8.png)\n",
    "\n",
    "> 더 많은 태스크를 가진 데이터 학습\n",
    "- 대부분의 경우와 마찬가지로, 데이터와 모델의 크기가 핵심이다.\n",
    "- Super-NaturalInstructions 데이터셋은 1.6K+의 태스크와 3M+의 예시로 구성되어 있음\n",
    "- 질문: 이러한 데이터로 학습한 거대 언어모델은 어떻게 평가해야 하는가?\n",
    "\n",
    "![image](./img/l9.png)\n",
    "\n",
    "> Instruction-tuned된 언어모델 평가 벤치마크\n",
    "- Massive Multitask Language Understanding(MMLU)\n",
    "- 57개의 지식을 요구하는 태스크에서 언어모델의 성능을 평가하는 벤치마크\n",
    "\n",
    "![image](./img/l10.png)\n",
    "\n",
    "> MMLU 예시\n",
    "- 천문학 문제\n",
    "  - 문제: Type-Ia 초신성에 대해 옳은 것은?\n",
    "  1. 이 유형은 쌍성계(binary systems)에서 발생한다.\n",
    "  2. 이 유형은 젊은 은하에서 발생한다.\n",
    "  3. 이 유형은 감마선 폭발을 낸다.\n",
    "  4. 이 유형은 많은 양의 X선을 나타낸다.\n",
    "\n",
    "> K-MMLU\n",
    "\n",
    "![image](./img/l11.png)\n",
    "![image](./img/l12.png)\n",
    "\n",
    "> MMLU에서의 발전\n",
    "- 지식 중심 벤치마크에서 빠르고 눈에 띄는 성과를 거두고 있다.\n",
    "\n",
    "![image](./img/l13.png)\n",
    "\n",
    "> 최근 거대 언어모델들\n",
    "\n",
    "![image](./img/l14.png)\n",
    "![image](./img/l15.png)\n",
    "![image](./img/l16.png)\n",
    "\n",
    "> 이로부터 우리는 무엇을 배웠는가?\n",
    "- 지시문(instructions), 입력(inputs), 출력(outputs)을 언어모델로부터 생성\n",
    "  - Alpaca: LLaMA 7B 모델을 52K개의 instruction-following 데이터로 파인튜닝\n",
    "- Instruction-tuning에는 많은 데이터가 필요하지 않음\n",
    "\n",
    "![image](./img/l17.png)\n",
    "![image](./img/l18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34df6c31",
   "metadata": {},
   "source": [
    "## 3. Reinforcemen Learning from Human Feedback(RLHF)\n",
    "> Instruction-tuning의 한계\n",
    "- 명확한 한계점 중 하나는 \"정답 데이터를 수집하는 것이 비싸다\"인데, 명확한 한계점 말로 다른 문제들은 뭐가 있을까?\n",
    "- 문제 1: 개방형/창의적 생성과 같은 태스크에는 정답이 존재하지 않는다.\n",
    "  - ex. \"개와 개가 기르는 메뚜기에 대한 이야기를 만들어줘\"\n",
    "- 문제 2: 언어 모델링은 모든 토큰 레벨의 오류를 동일하게 취급하지만, 어떤 오류는 다른 오류보다 심하게 작용한다.\n",
    "- 문제 3: 사람이 만든 답변(정답 레이블)이 최적이 아닐 수 있다.\n",
    "\n",
    "\n",
    "- Instruction-tuning을 하더라도, 언어모델의 목표(LM ovjective)와 \"인간의 선호를 만족시키는 것\" 사이에는 여전히 불일치가 존재한다.\n",
    "- 그렇다면, 인간의 선호(Human Preference)를 만족시키기 위해서 어떻게 해야할까요?\n",
    "\n",
    "![img](./img/l19.png)\n",
    "\n",
    "> 인간의 선호도(Human Preference)란?\n",
    "- 단답형 질문\n",
    "  - Q: 상대성 이론을 발표한 과학자는 누구인가?\n",
    "  - A: 알베르트 아인슈타인\n",
    "- 주관식 질문\n",
    "  - Q: 아인슈타인이 오늘날 살아있었다면, 현대 과학기술에 어떤 기여를 했을까?\n",
    "\n",
    "![img](./img/l20.png)\n",
    "\n",
    "> Intuition: Human-in-the-look ML\n",
    "\n",
    "![img](./img/l21.png)\n",
    "\n",
    "> 인간의 선호를 반영한 최적화(Optimizing for human preferences)\n",
    "- 언어모델을 학습한다고 가정했을 때 (e.g., 요약 태스크)\n",
    "- 어떤 지시문(instruction) x와 언어모델의 응답 y가 있을 때, 해당 요약에 대한 인간이 준 보상(human reward)을 얻을 수 있다고 상상해보자: $R(x, y) \\in \\mathbb{R}$, 점수가 높을수록 좋음\n",
    "\n",
    "![img](./img/l22.png)\n",
    "\n",
    "- 이제 우리는 언어모델의 응답 중 기대 보상(expected reward)를 최대화하는 것을 목표로 한다\n",
    "\n",
    "![img](./img/l23.png)\n",
    "\n",
    "> RLHF 파이프라인 큰 그림\n",
    "- 1단계: Instruction-tuning\n",
    "- 2, 3단계: 보상 최대화\n",
    "\n",
    "![img](./img/l24.png)\n",
    "\n",
    "![img](./img/l25.png)\n",
    "\n",
    "> 인간의 선호도(Human preference)를 어떻게 모델링할 것인가?\n",
    "- 문제: 인간의 판단은 일관성이 떨어지고, 기준이 어긋날 수 있다.\n",
    "- 해결 방법: 직접 점수를 매기지 않고, 응답을 비교하는 방식을 활용한다.\n",
    "- 해당 데이터로 리워드 모델(Reward model)을 학습시키게 된다.\n",
    "\n",
    "![img](./img/l26.png)\n",
    "\n",
    "- 사람이 보기에는 y2 > y1 > y3\n",
    "- 비교 결과가 리워드 모델 학습의 데이터가 됨\n",
    "\n",
    "> 어떻게 최적화를 할 것인가? ... 강화학습\n",
    "- 강화학습(Reinforcement Learning, RL) 분야는 오랫동안 이러한/관련된 문제들을 연구해왔음\n",
    "- 2013년 전후로, RL이 딥러닝과 게임 플레이에 적용되면서 다시 급증\n",
    "- 언어모델에 RL을 적용하려는 관심은 비교적 최근인 2019년부터 이루어짐\n",
    "  - RL을 언어모델에 적용하는 것이 어렵게 여겨짐 (현재도 마찬가지)\n",
    "  - 언어모델에 적용될 수 있는 RL 알고리즘이 최근 나옴\n",
    "\n",
    "\n",
    "> RLHF는 pre-training과 instruction-tuning보다 더 나은 성능 향상을 제공한다.\n",
    "\n",
    "![img](./img/l27.png)\n",
    "\n",
    "> 주의점: 리워드 모델이 잘 작동하는지 먼저 잘 확인해야 한다.\n",
    "\n",
    "![img](./img/l28.png)\n",
    "\n",
    "> ChatGPT: Instruction-tuning + RLHF\n",
    "- ChatGPT는 RLHF를 통한 발전에 힘입어 거대언어모델 중 처음으로 상업적으로 확용되었음\n",
    "\n",
    "![img](./img/l29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f70e27",
   "metadata": {},
   "source": [
    "## 4. What's Next?\n",
    "> 리워드 모델링의 한계점\n",
    "- 인간의 선호도는 일관성이 부족함\n",
    "  - 리워드 해킹(Reward Hacking)은 강화학습에서 자주 발생하는 문제이다.\n",
    "  - 챗봇들은 정답의 여부와 관계없이 생산적이고 도움이 되어 보이는 정답을 생성하게 된다.\n",
    "  - 이러한 결과들은 환각(Hallucination) 문제를 발생시키게 된다.\n",
    "- 인간의 선호도를 학습한 리워드 모델은 더 일관성이 부족함!\n",
    "\n",
    "![img](./img/l30.png)\n",
    "\n",
    "> RLHF에서 \"RL\"을 제거하자\n",
    "- Direct Preference Optimization(DPO); Rafailov et al. 2023\n",
    "\n",
    "![img](./img/l31.png)\n",
    "\n",
    "> RLHF에서 수학 문제와 같이 답이 분명한 문제들은 정답 여부로 리워드를 주자\n",
    "- Reinforcement Learning with Verifiable Reward(RLVR); Lambert et al, 2024, Deepseek-AI, 2025\n",
    "  - 대표적인 성공 사례: DeepSeek-R1\n",
    "\n",
    "![img](./img/l32.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52332457",
   "metadata": {},
   "source": [
    "# ⭐ <span style=\"background-color: #D9E5FF\">Retrieval-augmented Language Models (Information Retrieval, RAG)</span>\n",
    "\n",
    "> Contents\n",
    "1. Basic Concepts of Retrieval-augmented LM\n",
    "2. Information Retrieval\n",
    "3. Retrieval-augmented LM\n",
    "\n",
    "> 학습 목표\n",
    "- 검색증강 언어모델(Retrieval-augmented LM)에서 사용하는 용어들에 대해 이해한다.\n",
    "- 정보 검색(Information Retrieval)에서 사용되는 검색 메서드들을 이해한다.\n",
    "- 검색증강 언어모델(Retrieval-augmented LM)을 이해한다.\n",
    "\n",
    "## 0. 학습 시작(오버뷰)\n",
    "> 검색증강 언어모델(Retrieval-augmented LM)이 무엇일까?\n",
    "- Retrieval-augmented LM에서 사용되는 용어들에 대한 이해\n",
    "> Information Retrieval이 무엇일까?\n",
    "- 정보 검색의 활용에 대한 이해와 검색기(Retriever) 이해\n",
    "> Retrieval-augmented LM은 언제 사용해야 할까?\n",
    "- Retrieval-augmented LM이 필요한 이유와 도전과제들 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff59d40",
   "metadata": {},
   "source": [
    "## 1. Basic Concepts of Retrieval-augmented LM\n",
    "> Retrieval-augmented LM이란?\n",
    "- 추론 시 외부 데이터 저장소를 불러와 활용하는 언어모델\n",
    "\n",
    "![img](./img/l33.png)\n",
    "\n",
    "> Retrieval-augmented LM 구성요소\n",
    "- Datastore, Query, Index, Language Model\n",
    "\n",
    "![img](./img/l34.png)\n",
    "\n",
    "> Datastore\n",
    "- 가공되지 않은 대규모 텍스트 코퍼스\n",
    "  - 최소 수십억에서 수조 단위의 토큰으로 구성\n",
    "  - 라벨링된 데이터셋이 아님\n",
    "  - 지식베이스(Knowledge base)와 같은 구조화된 데이터가 아님\n",
    "\n",
    "> 쿼리(Query)\n",
    "- 검색 질의 / Retrieval input\n",
    "  - 언어모델의 질의(input)와 같아야 하는 것은 아님\n",
    "\n",
    "> 인덱스(Index)\n",
    "- 문서나 단락과 같은 검색 가능한 항목들을 체계적으로 정리하여 더 쉽게 찾을 수 있도록 하는 것\n",
    "- 각 정보 검색(Information Retrieval) 메서드는 인덱싱 과정에서 구축된 인덱스를 활용해, 쿼리와 관련있는 정보를 식별함\n",
    "\n",
    "> 검색(Retrieval)\n",
    "- Datastore에 있는 수 많은 정보 중에서, 주어진 쿼리(query)와 가장 관련성이 높은 정보를 찾아내는 과정\n",
    "\n",
    "> Retrieval-augmented Generation (RAG)\n",
    "- 사용자의 질문에 답하기 위해, datastore에서 관련 정보를 검색(Retrieval)해와서, 이를 언어모델이 생성(Generation) 단계에 함계 활용하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b54b6a8",
   "metadata": {},
   "source": [
    "## 2. Information Retrieval\n",
    "> Information Retrieval(정보검색)이란?\n",
    "- 목표: 검색 질의와 가장 관련성 높은 정보 제공\n",
    "- 사용자가 \"녹차의 효능?\"을 검색 -> IR이 관련 문서를 찾아 제공\n",
    "\n",
    "- 정보검색(IR): 사용자의 질의(Query)에 맞는 정보를 대규모 데이터에서 찾아 제공하는 과정\n",
    "- 주요 목표는 사용자의 검색 질의(Query)에 가장 관련성이 높은 정보를 제공하는 것\n",
    "\n",
    "![img](./img/l35.png)\n",
    "\n",
    "> 정보검색(IR)의 활용\n",
    "- 웹 서치(Web Search) & 아이템 서치(Item Search)\n",
    "  - 서치 엔진(Search engines)(e.g., 구글, 네이버)\n",
    "  - 이커머스(E-Commerce)(e.g., 아마존, 쿠팡)\n",
    "- 추천 시스템(Recommender System)\n",
    "  - OTT 서비스\n",
    "  - 이커머스(E-Commerce)\n",
    "- 검색증강생성(Retrieval-augmented Generation; RAG)\n",
    "\n",
    "![img](./img/l36.png)\n",
    "\n",
    "> Retriever이란?\n",
    "- 사용자의 질의(Query)에 맞는 후보 문서를 저장소에서 찾아오는 모듈\n",
    "- 검색 증강 언어모델(RAG)에서 첫 단계 역할을 수행\n",
    "\n",
    "> Retriever 종류\n",
    "- Sparse Retriever(어휘적 유사도 기반)\n",
    "  - 전통적인 정보검색(IR) 기법으로, 쿼리(query)와 문서 간의 정확한 용어 일치(즉, 어휘적 유사도)에 기반한다.\n",
    "  - 예시: TF-IDF, BM-25\n",
    "- Dense Retriever(의미적 유사도 기반)\n",
    "  - 쿼리와 문서를 표현하기 위해 dense vector(a.k.a, embeddings; 임베딩)을 활용해 의미적 유사도에 기반한다.\n",
    "  -예시: DPR, Contriever, Openai-embeddings, etc\n",
    "\n",
    "![img](./img/l37.png)\n",
    "\n",
    "> Sparse Retriever - TF-IDF(Term Frequency-Inverse Document Frequency)\n",
    "- 문서 내 특정 단어의 중요도를 나타내는 가중치 방식\n",
    "  - TF(Term Frequency): 단어가 문서에 얼마나 자주 등장하는지\n",
    "  - IDF(Inverse Document Frequency): 단어가 전체 코퍼스에서 얼마나 드물게 등장하는지\n",
    "- <span style=\"color: #87CEFA\">TF</span> -> 문서 안에서 <span style=\"color: #87CEFA\">자주 등장하는 단어는 중요</span>하다.\n",
    "- <span style=\"color: #FFA500\">IDF</span> -> 하지만 <span style=\"color: #FFA500\">너무 많은 문서에 등장하는 단어는 덜 중요</span>하다.\n",
    "  - \"this, is, and\"과 같은 단어들은 IDF 값이 낮아져 가중치가 0에 가까워진다.\n",
    "\n",
    "![img](./img/l38.png)\n",
    "\n",
    "> Sparse Retriever - 장/단점\n",
    "- 장점\n",
    "  - 단순성(Simplicity): 구현과 이해가 비교적 쉽다\n",
    "  - 효율성(Efficiency): Inverted index 구조 덕분에 빠른 검색과 효율적인 질의 처리가 가능하다\n",
    "  - 투명성(Transparency): 검색 결과가 보통 해석 가능하며, 용어 매칭에 기반하기 때문에 설명이 명확\n",
    "- 단점\n",
    "  - 제한된 의미 이해(Limited semantic understading): 아래 예시처럼, 쿼리에는 \"bad guy\"라는 표현이 쓰였지만, 실제 문서에는 \"villain\"이라는 단어가 사용되어 매칭되지 않음\n",
    "    - 쿼리: \"Who is the bad guy i the movie lord of the rings?\"\n",
    "    - 문서: \"Sala Vaker is bet known for portraying the villain Sauron in the lord of the rings trilogy\"\n",
    "\n",
    "> Dense Retriever - 임베딩 모델(Embedding Models)\n",
    "- 임베딩 모델(e.g., BERT)은 단어/문장의 의미를 표현할 수 있음\n",
    "\n",
    "![img](./img/l39.png)\n",
    "\n",
    "> Dense Retriever - Bi-encoder\n",
    "- Bi-encoder retriever는 대조 학습(contrastive learning)을 통해 학습되며, 이는 쿼리가 긍정적인 문서와 가깝게 유지되도록 하고 부정적인 문서에서는 멀어지도록 유도한다.\n",
    "\n",
    "![img](./img/l40.png)\n",
    "\n",
    "> Dense Retriever - Cross-encoder\n",
    "- Cross-encoder 아키텍처는 두 개의 텍스트를 하나의 시퀀스로 결합\n",
    "  - e.g.,  \"query [SEP] document\"\n",
    "- Self-attention을 통해 모든 쿼리와 문서 토큰이 완전히 상호작용할 수 있어, bi-encoder보다 더 높은 정확도를 얻을 수 있다.\n",
    "- 하지만 모든 쿼리-문서 쌍을 개별적으로 모델에 입력해야 하므로, 계산 비용이 크고 처리속도가 느리다는 단점이 있다.\n",
    "\n",
    "![img](./img/l41.png)\n",
    "\n",
    "> Dense Retriever - Bi-encoder vs Cross-encoder\n",
    "- Bi-encoder는 두 문장을 따로 인코딩한다.\n",
    "  - 매우 빠르고 대규모 데이터베이스 검색에 적합하지만 정확도는 낮다.\n",
    "- Cross-encoder는 두 문장을 함께 처리한다.\n",
    "  - 매우 느리지만, 세밀한 상호작용을 포착하기 때문에 정확도는 높다.\n",
    "\n",
    "![img](./img/l42.png)\n",
    "\n",
    "> Dense Retriever - 장단점\n",
    "- 장점\n",
    "  - 의미적 이해(Semantic understanding): Dense retriever는 동의어나 다양한 표현을 더 효과적으로 처리할 수 있으며, 즉 글의 문맥과 의미를 포착한다.\n",
    "  - 풍부한 쿼리 표현(Rich query representation): 복잡한 쿼리와 긴 검색 쿼리의 의미를 더 잘 포착할 수 있다.\n",
    "- 단점\n",
    "  - 높은 연산 비용(High computational cost): 모델 학습에는 상당한 계산 자원과 시간이 필요하며, 추론 과정에서도 많은 자원을 소모할 수 있다.\n",
    "  - 제한된 투명성(Limited transparency): Dense retriever는 블랙박스처럼 작동할 수 있어, 특정 문서가 왜 검색되었는지 해석하기 어렵다.\n",
    "  - 데이터 및 모델 의존성(Dependency on data and models): Dense retriever의 성능은 학습 데이터의 품질이나 모델 변경에 크게 영향을 받는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96804990",
   "metadata": {},
   "source": [
    "## 3. Retrieval-augmented LM\n",
    "> Retrieval-augmented LM이란?\n",
    "- 추론 시 외부 데이터 저장소를 불러와 활용하는 언어모델을 RAG라고 부름\n",
    "- RAG(Retrieval-augmented Generation): 정보 검색부터 답변 생성까지의 프레임워크\n",
    "\n",
    "![img](./img/l43.png)\n",
    "\n",
    "> Why Retrieval-augmented LM?\n",
    "1. 거대언어모델은 모든 지식을 다 자신의 파라미터에 저장하지 못한다\n",
    "  - 거대 언어모델은 pre-training(사전학습) 데이터에 자주 나타나는 쉬운 정보를 기억하는 경향성이 있음 (Kandpal et al. 2022)\n",
    "  - RAG는 자주 등장하지 않는 정보에 대해서 큰 효과를 가져다 줌; 언어 모델이 잘 알고 있는 정보인 경우, 부정적인 영향을 미칠 수도 있음 (Mallen et al. 2023)\n",
    "\n",
    "![img](./img/l44.png)\n",
    "\n",
    "2. 거대언어모델이 보유한 지식은 금세 시대에 뒤쳐지며, 갱신이 어렵다.\n",
    "  - 현재의 지식 편집(knowledge editing) 메서드들은 확장성이 부족함\n",
    "  - 반면에, 저장소(Datastore)는 쉽게 업데이트가 가능하며, 확장성도 만족함\n",
    "\n",
    "![img](./img/l45.png)\n",
    "\n",
    "3. 거대언어모델의 답변은 해석과 검증이 어려움\n",
    "  - 현재의 지식 편집(knowledge editing) 메서드들은 확장성이 부족함\n",
    "  - 반면에, 저장소(Datastore)는 쉽게 업데이트가 가능하며, 확장성도 만족함\n",
    "\n",
    "![img](./img/l46.png)\n",
    "\n",
    "4. 기업 내부 정보와 같은 보안 정보는 언어모델 학습에 활용되지 않음\n",
    "  - 사내 챗봇/기업 내부 시스템에 언어모델을 사용하는 경우, 내부 데이터를 학습 시 정보 유출의 위험성이 있음\n",
    "\n",
    "> Retrieval-augmented LM 파이프라인\n",
    "- 검색증강 언어모델: 언어모델에 질문과 더불어 검색엔진 결과를 함께 이용\n",
    "\n",
    "![img](./img/l47.png)\n",
    "![img](./img/l48.png)\n",
    "\n",
    "> Challenges of Retrieval-augmented LM?\n",
    "1. Context를 어떻게 구성해야 하는가?\n",
    "  -> Solution: 언어모델의 컨텍스트 길이(Context length)를 늘려야 한다.\n",
    "    <br> Position embedding, Efficient inference, Fine-tuning on long context ...\n",
    "  - Claude 3 모델들은 200K context length/window를 제공한다.\n",
    "  - Gemini 1.5 Pro는 128,000 token context window를 제공한다.\n",
    "\n",
    "2. RAG의 결과는 검색 모델 성능에 의존\n",
    "  <br> -> 검색 노이즈에 취약\n",
    "    - 정확하지 않은 유사 정보가 언어모델이 답하는 것을 방해할 수 있음\n",
    "  <br>:Context 안의 정보를 이용하려는 LLM의 경향성 -> 검색에서의 노이즈가 Hallucination을 증가시킴\n",
    "\n",
    "![img](./img/l50.png)\n",
    "![img](./img/l51.png)\n",
    "![img](./img/l52.png)\n",
    "\n",
    "> Noise Robustness\n",
    "- 외부 문서에 노이즈(관련 없는 정보)가 포함되어 있어도 올바른 답을 찾아내는 능력\n",
    "  - 올바른 문서: 2022년 수상자는 Annie Ernaux\n",
    "  - 노이즈 문서: 2021년 수상자는 Abdulrazak Gurnah\n",
    "- RAG 동작\n",
    "  - 문서 안에 노이즈가 있어도 올바른 정보를 식별하고 정답을 생성.\n",
    "  - 최종답변: \"Annie Ernaux\"\n",
    "\n",
    "![img](./img/l53.png)\n",
    "\n",
    "> Negative Rejection\n",
    "- 질문: \"Who was awarded the 2022 Nobel prize in Literature\"\n",
    "- 외부 문서\n",
    "  - 2021년 수상자 Abdulrazak Gurnah\n",
    "  - 2020년 수상자 Louise Glück\n",
    "- RAG 동작\n",
    "  - 검색한 문서에 2022년 관련 정보가 없음\n",
    "  - 따라서 모델은 추측하지 않고 명확히 답변을 거부\n",
    "- 최종 답변:\n",
    "  - 대답할 수 없음\n",
    "\n",
    "![img](./img/l54.png)\n",
    "\n",
    "> Challenges of Retrieval-augmented LM?\n",
    "3. LLM의 사전지식과 컨텍스트 간의 충돌(Conflict) 발생\n",
    "\n",
    "![img](./img/l55.png)\n",
    "\n",
    "-> Solution 1: Context 위에서 Grounding 학습 강화\n",
    "\n",
    "![img](./img/l56.png)\n",
    "\n",
    "-> Solution 1: Context가 없을 때, 답변 회피/거절 학습\n",
    "\n",
    "![img](./img/l57.png)\n",
    "\n",
    "\n",
    "4. 복잡한 추론 필요 & 문서가 명확한 사실에 대한 오류를 포함할 때\n",
    "\n",
    "![img](./img/l58.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3654df17",
   "metadata": {},
   "source": [
    "# ⭐ <span style=\"background-color: #D9E5FF\">LLMs with Tool Usage(LLM Agent, Tool Use, MCP)</span>\n",
    "\n",
    "> Contents\n",
    "1. Basic Concepts of LLM Agents\n",
    "2. Tool Usage in LLMs\n",
    "3. Model Context Protocol(MCP)\n",
    "\n",
    "> 학습 목표\n",
    "- LLM Agent에 대한 기본 개념을 이해한다.\n",
    "- LLM의 외부 툴 사용에 대한 개념을 이해한다.\n",
    "- MCP 개념을 이해한다.\n",
    "\n",
    "## 0. 학습 시작(오버뷰)\n",
    "> LLM Agent이란 무엇일까?\n",
    "- Agent에 대한 개념과 LLM Agent를 구성하는 요소들을 이해한다.\n",
    "> LLM이 어떻게 외부 툴을 사용하게 되었을까?\n",
    "- 언어모델이 어떻게 툴을 사용하는 방법을 배우는지 이해한다.\n",
    "> MCP란 무엇일까?\n",
    "- MCP의 등장하게 된 배경과 개념을 이해한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2486ccb",
   "metadata": {},
   "source": [
    "## 1. Basic Concepts of LLM Agents\n",
    "> Agent이란?\n",
    "- 센서를 통해 환경(environment)을 인지하고, 액추에이터(Actuator)를 통해 환경에 대해 액션(action)을 통해 영향을 미치는 것으로 간주될 수 있는 모든 것\n",
    "\n",
    "![img](./img/l59.png)\n",
    "\n",
    "> 강화학습(Reinforcement Learning) - 의사결정의 과학\n",
    "\n",
    "![img](./img/l60.png)\n",
    "\n",
    "> LLM Agent란?\n",
    "- 거대언어모델(LLM)을 핵심 구조(backbone)로 삼아 환경을 이해하고 행동을 수행하는 에이전트\n",
    "- LLM-first view: 기존 LLM을 활용한 시스템을 에이전트로 만든다.\n",
    "  - 서치(search) 에이전트, 심리상담 에이전트, 코드(Code) 에이전트\n",
    "- Agent-first view: LLM을 AI 에이전트에 통합하여, 언어를 활용한 추론과 의사소통을 가능하게 한다.\n",
    "  - 로봇, 임바디드(embodied) 에이전트\n",
    "\n",
    "![img](./img/l61.png)\n",
    "\n",
    "> 이것은 에이전트인가?\n",
    "- 웹을 탐색하는 LLM 시스템 - Yes\n",
    "- OS의 파일을 검색하고 코드를 사용해 처리하는 LLM 시스템 - Yes\n",
    "- 정보를 검색(Retrieve)하고 생성(generate)하는 LLM 시스템 - Probably not(점진적이지 않음)\n",
    "- 복잡한 추론을 하는 O1 같은 LLM - No(툴도 없고 외부 환경과 상호작용하지 않음)\n",
    "\n",
    "> 성공적인 에이전트가 갖춰야 할 요건들\n",
    "- 도구 사용 (Tool use)\n",
    "- 추론과 계획 (Reasoning and Planning)\n",
    "- 환경 표현 (Environment Representation)\n",
    "- 환경 이해 (Environment understanding)\n",
    "- 상호작용/의사소통 (Interaction/Communication)\n",
    "\n",
    "> LLM Agent 프레임워크\n",
    "\n",
    "![img](./img/l62.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37a2c6a",
   "metadata": {},
   "source": [
    "## 2. Tool Usage in LLMs\n",
    "> Tool이란? (LLM 에이전트를 위한)\n",
    "- 언어모델 외부(external)에서 실행되는 프로그램에 연결되는 함수(function) 인터페이스를 의미한다.\n",
    "  - LLM은 함수 호출과 입력 인자를 생성함으로써 이 도구를 활용할 수 있다.\n",
    "\n",
    "![img](./img/l63.png)\n",
    "\n",
    "> 도구 사용 패러다임(Tool Use Paradigms)\n",
    "- 도구 사용(Tool Use): 두 모드 간의 전환\n",
    "  - 텍스트 생성 모드(text-generation mode)\n",
    "  - 도구 실행 모드(tool-execution mode)\n",
    "- 도구 사용을 유도하는 방법\n",
    "  - 추론 시 프롬프트 (inference-time prompting)\n",
    "  - 학습 (Training; Tool Learning)\n",
    "\n",
    "![img](./img/l64.png)\n",
    "\n",
    "> 툴 러닝(Tool Learning) 방식\n",
    "- 모방 학습(Imitation Learning): 인간의 도구 사용 행동 데이터를 기록함으로써, 언어모델이 인간의 행동을 모방하도록 학습\n",
    "- 가장 간단하고 직관적인 방식\n",
    "\n",
    "![img](./img/l65.png)\n",
    "\n",
    "- OpenAI: WebGPT(2022)\n",
    "  - 검색 엔진을 사용하기 위해 인간의 행동 모방(Clone human behavior)\n",
    "  - 지도 학습(Supervised fine-tuning) + 강화학습(Reinforcement Learning)\n",
    "  - 단, 6000개의 데이터만 필요\n",
    "\n",
    "![img](./img/l66.png)\n",
    "\n",
    "> 툴 러닝(Tool Learning) 방식 - 모방학습(Imitation Learning)\n",
    "- OpenAI: WebGPT(2022)\n",
    "  - 장문 질의응답(Long-form QA)에서 뛰어난 성능을 보이며, 인간보다 좋은 성능을 보이기도 한다.\n",
    "\n",
    "![img](./img/l67.png)\n",
    "\n",
    "- Meta: Toolformer (2023)\n",
    "  - 모델 스스로 학습 데이터 생성 (Self-supervised)\n",
    "  - 지도 학습(Supervised fine-tuning)\n",
    "  - 검색 엔진 뿐만 아니라 달력, 계산기와 같은 여러 API를 활용\n",
    "\n",
    "![img](./img/l68.png)\n",
    "\n",
    "- Meta: Toolformer (2023) - 데이터 생성 방식\n",
    "1. API 호출 샘플링: LLM이 기존 데이터셋에서 문맥을 보고 API 호출 후보를 생성\n",
    "2. API 실행: 생성된 API 호출을 실제로 실행하여 응답을 얻음\n",
    "3. API 호출 필터링: API 호출이 문맥에 유용한지 평가 및 필터링 진행\n",
    "\n",
    "![img](./img/l69.png)\n",
    "\n",
    "- ToolLLM(2023)\n",
    "  - 기존 연구의 한계점인 툴의 다양성과 범용성을 타겟팅한 연구\n",
    "  - 16,000+의 실제 api를 활용하여 대규모 학습 데이터 셋 & 특화 언어모델 & 평가 벤치마크를 제안\n",
    "\n",
    "- ToolLLM(2023) - 데이터셋 수집 방법\n",
    "  - API를 수집\n",
    "  - 수집한 API를 기반하여 명령문(Instruction) 생성\n",
    "  - 정답 어노테이션 & 필터링\n",
    "\n",
    "![img](./img/l70.png)\n",
    "\n",
    "> 툴 러닝(Tool Learning) 최근 접근 - 멀티 모달 툴 러닝(Multi-modal Tool Learning)\n",
    "- 멀티모달 대규모 언어모델(MLLM)을 기반으로 도구를 정의하고 활용하는 연구\n",
    "- ex. GUI 에이전트, Embeddied 에이전트\n",
    "\n",
    "![img](./img/l71.png)\n",
    "\n",
    "> 툴 러닝(Tool Learning) 최근 접근 - 강화학습\n",
    "- 지도 학습을 넘어 에이전트에서의 강화학습을 도입하는 연구\n",
    "\n",
    "![img](./img/l72.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3e4e89",
   "metadata": {},
   "source": [
    "## 3. Model Context Protocol (MCP)\n",
    "> Why MCP?\n",
    "- 외부 툴을 활용하는 연구가 급증하면서 회사/모델마다 각기 다른 툴 호출 방식 및 스키마를 개발\n",
    "- 문제점:\n",
    "  - 호환성 부족(모델마다 다름)\n",
    "  - 재사용 어려움 (같은 툴도 다른 모델에서는 다시 정의해야 함)\n",
    "\n",
    "\n",
    "> MCP란?\n",
    "- 언어모델이 외부 툴과 상호작용하기 위한 표준화된 방식으로 정의한 프로토콜\n",
    "- 툴 호출, 응답 전달, 컨택스트 공유를 하나의 공통 규격으로 처리\n",
    "\n",
    "** 보안이 약함(뚫리기 좋음)**\n",
    "\n",
    "![img](./img/l73.png)\n",
    "\n",
    "> MCP 아키텍쳐\n",
    "- MCP Host: 하나 또는 여러 개의 MCP 클라이언트를 조정하고 관리하는 AI 어플리케이션\n",
    "- MCP Client: MCP 서버와의 연결을 유지하며 MCP 호스트가 사용할 수 있도록 MCP 서버로부터 컨택스트를 가져오는 구성요소\n",
    "- MCP ServerL MCP 클라이언트에게 컨텍스트를 제공하는 프로그램\n",
    "\n",
    "![img](./img/l74.png)\n",
    "\n",
    "> MCP 계층(Layer)\n",
    "- MCP는 두 개의 계층으로 구성된다\n",
    "  - 데이터 게층(Data Layer): 클라이언트-서버 통신을 위한 JSON-RPC 기반 프로토콜을 정의\n",
    "    - 라이프 사이클 관리, 툴, 리소스, 프롬프트와 같은 핵심 요소들이 포함된다.\n",
    "  - 전송 계층(Transport Layer): 클라이언트 서버 간 데이터 교환을 가능하게 하는 통신 메커니즘과 채널을 정의\n",
    "    - 전송 방식에 특화된 연결 수립, 메세지 프레이밍, 인증이 포함된다.\n",
    "- 개념적으로 데이터 계층은 내부 계층(inner layer), 전송 계층은 외부 계층(outer layer)이다.\n",
    "\n",
    "> 예시: MCP 이전 / 이후 툴 사용 요청(tool execution request) 비교\n",
    "- 단순화된 예시: get_weather() 함수\n",
    "\n",
    "![img](./img/l75.png)\n",
    "\n",
    "> MCP의 장점\n",
    "- 표준화(Standardization): 모든 모델/툴이 동일한 호출 규격 사용\n",
    "- 확장성(Extensibility): 새로운 툴 쉽게 추가 가능\n",
    "- 호환성(Interoperability): 모델/플랫폼에 상관없이 같은 툴 호출 가능\n",
    "- 재사용성(Reusability): 한 번 정의한 툴을 여러 모델에서 활용 가능\n",
    "- 투명성(Transparency): 호출 과정이 명확히 기록/검증됨\n",
    "\n",
    "> MCP 서버 예시 코드\n",
    "- FastMCP 라이브러리 사용\n",
    "- 누구나 간단하게 사용 가능\n",
    "\n",
    "![img](./img/l76.png)\n",
    "\n",
    "> 현재\n",
    "- MCP는 현재 학계/업계에서도 모두 표준으로 확립\n",
    "- Web과 AI 개발을 구분하게 되어 양쪽 생태계 모두에서 표준적이고 호환 가능한 개발이 가능해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0da50e0",
   "metadata": {},
   "source": [
    "# ⭐ <span style=\"background-color: #D9E5FF\">AI Agents & Langchain (AI Agents, Langchain)</span>\n",
    "\n",
    "> Contents\n",
    "1. Environemnt Representation & Understanding\n",
    "2. Reasoning & Planning\n",
    "3. Langchain\n",
    "\n",
    "> 학습 목표\n",
    "- 에이전트가 상호작용을 하는 환경은 어떻게 표현되는지와 LLM은 이를 어떻게 이해하는지 알아본다\n",
    "- 에이전트가 태스크를 수행하기 위해 추론/계획을 어떻게 수립하는지 이해한다\n",
    "- Langchain이 무엇인지 이해한다\n",
    "\n",
    "## 0. 학습 시작(오버뷰)\n",
    "> 에이전트가 상호작용하는 환경이란 무엇일까?\n",
    "- 태스크 환경에 대한 이해와 언어모델이 어떻게 환경을 이해하는지에 대해 사례 중심으로 이해한다.\n",
    "> 에이전트가 추론/계획은 어떻게 할까?\n",
    "- 사용자의 요청을 수행하기 위해 에이전트는 어떻게 추론하고 계획을 수립하는지 사례 중심으로 이해한다.\n",
    "> Langchain이 무엇일까?\n",
    "- Langchain이 무엇인지 이해하고, 어디서 시작하면 되는지 안다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb683af6",
   "metadata": {},
   "source": [
    "## 1. Environment Representation & Understanding\n",
    "\n",
    "> Recap: LLM Agent란?\n",
    "- 거대언어모델(LLM)을 핵심구조(backbone)로 삼아 환경을 이해하고 행동을 수행하는 에이전트\n",
    "- LLM-first view: 기존 LLM을 활용한 시스템을 에이전트로 만든다.\n",
    "  - 서치(search) 에이전트, 심리상담 에이전트, 코드(Code) 에이전트\n",
    "- Agent-first view: LLM을 AI 에이전트에 통합하여, 언어를 활용한 추론과 의사소통을 가능하게 한다.\n",
    "  - 로봇, 임바디드(embodied) 에이전트\n",
    "\n",
    "> Recap: 성공적인 에이전트가 갖춰야 할 요건들\n",
    "- 도구 사용(Tool use)\n",
    "- 추론과 계획(Reasoning and Planning)\n",
    "- <span style=\"color: #6495ED\">환경 표현(Environment Representation)</span>\n",
    "- <span style=\"color: #6495ED\">환경 이해(Environment Understanding)</span>\n",
    "- 상호작용/의사소통(Interation/Communication)\n",
    "\n",
    "> LLM Agent 프레임워크\n",
    "\n",
    "![img](./img/l77.png)\n",
    "\n",
    "> 환경(Environment)과 에이전트 활용 예시들\n",
    "- 챗봇\n",
    "- 로보틱스\n",
    "- 임바디드(Embodied) 에이전트\n",
    "- 게임\n",
    "- 소프트웨어 개발\n",
    "\n",
    "> 에이전트가 환경을 이해하기 위해 필요한 것\n",
    "- 환경에 접근하기 위한 툴(Tool)\n",
    "- 환경의 표현(Representation)\n",
    "- 환경을 이해/탐색하기 위한 방법론들\n",
    "\n",
    "> 환경의 표현(Representation): 텍스트(Text)\n",
    "- ALFWorld; Shridhar et al. 2021\n",
    "  - 텍스트 기반 임바디드 에이전트 시뮬레이션 엔진\n",
    "  - 물리적 세계에 대한 정보를 언어모델이 텍스트 기반으로 이해하고 명령을 수행할 수 있도록 환경을 표현함\n",
    "\n",
    "![img](./img/l78.png)\n",
    "\n",
    "> 환경의 표현(Representation): 이미지(Image)\n",
    "- Touchdown(Chen et al., 2018)\n",
    "  - Google Street View 기반 내비게이션 및 지시 따르기 (Navigation & Instruction Following) 데이터셋\n",
    "  - 사용자가 제공한 자연어 지시문에 따라 실제 도시 환경 이미지 내에서 경로 탐색을 수행\n",
    "  - 시각적 환경을 텍스트와 연결하여 지시 수행 및 환경 이해를 가능하게 함\n",
    "\n",
    "![img](./img/l79.png)\n",
    "\n",
    "> 이미지 기반 표현의 문제점 (Problems w / Image Representations)\n",
    "- 에이전트로서 좋은 성능을 내려면 세부적인 이해가 중요\n",
    "  - 예: OCR(광학 문자 인식), 복잡한 레이아웃에서의 그라운딩\n",
    "- 많은 모델들이 이런 태스크에서 실패 하지만 일정 수준의 학습을 통해 개선이 가능하다.\n",
    "\n",
    "![img](./img/l80.png)\n",
    "\n",
    "> 환경의 표현(Representation): 텍스트 기반 웹(Textual Web Representations)\n",
    "- WebArena(Zhou and Xu et al., 2024)\n",
    "  - 웹 환경을 텍스트(HTML, DOM, Accessibility Tree)로 표현하여 에이전트가 상호작용 가능\n",
    "  - 단순한 스크린샷 이미지 대신 구조화된 웹 표현을 제공\n",
    "  - 이를 통해 에이전트가 버튼 클릭, 항목 선택, 같은 작업을 더 정확히 수행할 수 있음\n",
    "\n",
    "![img](./img/l81.png)\n",
    "\n",
    "> 복잡한 환경은 어떻게 이해할 수 있을까?\n",
    "- 모델은 자신이 상호작용하는 환경에 대해 모든 것을 알고있지 않음\n",
    "- 일부 지식은 LLM 파라미터 안에 포함되어 있음\n",
    "  - ex. 코딩 지식, 자주 사용하는 웹사이트 탐색 방법 등\n",
    "- 다른 지식은 실시간으로 환경과의 상호작용을 통해 발견해야 함\n",
    "\n",
    "> 복잡한 환경에 대한 이해: 환경 특화 프롬프트(Environment-soecific Prompts)\n",
    "- 환경에 맞게 수동으로 프롬프트를 제작하여 에이전트가 지시를 따르도록 유도\n",
    "  - ex. SteP(Sochi et al., 2023) - 웹 탐색을 위한 프롬프트 템플릿\n",
    "- 문제점: 일반화(Generalization)\n",
    "  - 특정 환경에는 잘 작동하지만, 새로운 환경에서는 성능이 떨어짐\n",
    "\n",
    "> 복잡한 환경에 대한 이해: 비지도 프롬프트 유도(Unsupervised Induction of Prompts)\n",
    "- Agent Workflow Memory(Wang et al., 2024)\n",
    "  - 성공적으로 수행된 워크플로우(workflow)를 기억하고, 이를 기반으로 새로운 프롬프트를 생성하여 에이전트에 제공\n",
    "  - 환경에서의 상호작용을 메모리(memory)에 통합\n",
    "- 프롬프트를 사람이 수동으로 설계하지 않고, 에이전트가 경험을 통해 자동 생성 및 일반화\n",
    "\n",
    "![img](./img/l82.png)\n",
    "\n",
    "> 복잡한 환경에 대한 이해: 환경 탐색(Environment Exploration)\n",
    "- 모델이 환경을 탐색할 떄 보상(reward)을 부여하여 학습을 유도 -> 호기심(curoisity) 기반 보상\n",
    "  - ex. 강화학습에서 예측 불가능한 상태 공간(state space)에 진입할 경우 보상을 증가\n",
    "\n",
    "> 복잡한 환경에 대한 이해: 탐색 기반 궤적 기억 (Exploration-based Trajectory Memorization)\n",
    "- BAGEL(Murtey et al., 2024)\n",
    "  - 명령어를 샘플링하여 실행한 뒤, 더 정확한 새로운 명령어로 궤적(trajectory)을 재라벨링\n",
    "  - LM-agent가 환경을 탐색하고, LM-labeler가 이를 바탕으로 지시를 다시 정제\n",
    "- 기존 데이터에 의존하지 않고, 탐색과 자기 교정(self-correction)을 통해 데이터 생성\n",
    "- 에이전트의 일반화 성능과 환경 적응력을 향상\n",
    "\n",
    "![img](./img/l83.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730013bb",
   "metadata": {},
   "source": [
    "# 2. Reasoning & Planning\n",
    "\n",
    "> Recap: 성공적인 에이전트가 갖춰야 할 요건들\n",
    "\n",
    "- 도구 사용(Tool use)\n",
    "- <span style=\"color: #6495ED\">추론과 계획(Reasoning and Planning)</span>\n",
    "- 환경 표현(Environment Representation)\n",
    "- 환경 이해(Environment Understanding)\n",
    "- 상호작용/의사소통(Interation/Communication)\n",
    "\n",
    "> LLM Agent 프레임워크\n",
    "\n",
    "![img](./img/l84.png)\n",
    "\n",
    "> Cotroller: Planning 종류\n",
    "- 국소적 계획(Local Planning)\n",
    "  - 한 단계씩(step by step) 계획을 세움\n",
    "  - 매 스텝마다 사용할 하나의 툴(tool) 결정\n",
    "  - 단순하고 직관적이나, 장기 의존성 문제 발생\n",
    "- 전역적 계획(Global Planning)\n",
    "  - 실행 가능한 전체 계획 경로(planning path)를 한 번에 생성\n",
    "  - 여러 개의 툴을 조합하여 시퀀스 형태로 결정\n",
    "  - 효율적이나, 복잡한 환경에서는 실패 가능\n",
    "\n",
    "![img](./img/l85.png)\n",
    "\n",
    "> Local planning\n",
    "- ReACT(Yap et al., 2023)\n",
    "  - 추론(reasoning)과 액션(action)을 결합하여 에이전트가 환경과 상호작용하도록 하는 방식\n",
    "  - 단순한 추론/행동 분리를 넘어, 두 과정을 경합해 상황 적응형(local) 계획 가능\n",
    "  - 다양한 도구 사용 및 웹 탐색, 추론 기반 Q&A 등에서 높은 성능\n",
    "\n",
    "![img](./img/l86.png)\n",
    "\n",
    "> Global planning\n",
    "- Plan-and-Solve Prompting(Wang et al., 2023)\n",
    "  - 전체 계획을 세운 뒤, 그 계획에 따라 순차적으로 문제를 해결\n",
    "  - 단순한 step-by-step 방식보다 체계적이고 안정적인 추론 가능\n",
    "  - 복잡한 문제 해결에서 오류 축적 감소\n",
    "\n",
    "![img](./img/l87.png)\n",
    "\n",
    "> 계획 재검토(Revisiting Plans)\n",
    "- CoAct(Hou et al., 2024)\n",
    "  - 에이전트가 실행 도중 계획을 재검토(revisit)하고 수정 가능\n",
    "  - 두 개의 에이전트가 서로 협력하여 오류가 발생 시 재계획과 피드백을 수행한다.\n",
    "- 초기 계획이 불완전하더라도 실행 과정에서 보완 가능\n",
    "- 장기 작업(Long-horizon tasks)에서 안정성과 성공률 향상\n",
    "\n",
    "![img](./img/l88.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9908a836",
   "metadata": {},
   "source": [
    "## 3. Langchain\n",
    "\n",
    "> Langchain이란?\n",
    "- LLM 기반 어플리케이션을 짜르게 개발할 수 있는 오픈소스 프레임워크\n",
    "- LLM을 다양한 데이터/툴과 연결하여 강력한 어플리케이션 개발 가능\n",
    "- 연구와 산업 현장에서 빠르게 표준으로 자리잡음\n",
    "\n",
    "![img](./img/l89.png)\n",
    "\n",
    "> Langchain 특징\n",
    "- 다양한 LLM provider(OpenAI, Anthropic, Google, etc)와 통합하여 모델/회사별 API 차이를 공통 인터페이스로 관리 가능\n",
    "- Prompt, Memory, Tools와 같은 컴포넌트들이 모듈화되어 있어 재사용성과 확장성 확보\n",
    "- LangGraph 기반으로 복잡한 워크플로우를 시각적으로 설계 및 관리 가능\n",
    "\n",
    "![img](./img/l90.png)\n",
    "\n",
    "> Langchain 주요 컴포넌트\n",
    "- Prompt Templates: 프롬프트를 구조화\n",
    "- Chains: 여러 단계를 연결한 워크 플로우\n",
    "- Agents: 동적으로 툴 선택 및 실행\n",
    "- Memory: 대화 히스토리와 상태 유지\n",
    "- Tools: 외부 API, DB, 계산기 등 연결\n",
    "- Etc ...\n",
    "\n",
    "![img](./img/l91.png)\n",
    "\n",
    "> Langchain 튜토리얼(https://python.langchain.com/docs/tutorials/)\n",
    "- Langchain은 다양한 튜토리얼이 잘 되어 있음\n",
    "- 이번 강의에서 배웠던 Retrieved-augmented LM, Agent 등이 다 포함되어 있다.\n",
    "\n",
    "> RAG w/ Langchain\n",
    "- 튜토리얼 링크: https://python.langchain.com/docs/tutorials/rag/\n",
    "- LLM 설정부터 인덱싱과 RAG 다 존재\n",
    "\n",
    "> Agent w/ Langchain\n",
    "- 튜토리얼 링크: https://python.langchain.com/docs/tutorials/agents/\n",
    "- ReACT 에이전트 셋업부터 실행까지 전반적인 흐름을 알려주는 튜토리얼\n",
    "- 활용할 수 있는 툴 리스트: https://python.langchain.com/docs/integrations.tools/\n",
    "\n",
    "> MCP w/ Langchain\n",
    "- 튜토리얼 링크: https://github.com/langchain-ai/langchain-mcp-adapters?tab=readme-ov-file\n",
    "- MCP를 langchain으로 활용할 수 있도록 최근 langchain에서 개발한 라이브러리\n",
    "- 자기만의 MCP 서버를 만들 수 있게 되었음\n",
    "\n",
    "![img](./img/l92.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
