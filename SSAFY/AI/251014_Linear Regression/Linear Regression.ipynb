{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aef2d69",
   "metadata": {},
   "source": [
    "# <span style=\"background-color: #FAFAD8\">AI & 기계학습 방법론 1<span>\n",
    "##### 선형회귀(Linear Regression)\n",
    "> CONTENTS\n",
    "1. 선형회귀: 입력과 출력의 선형 관계를 찾는 방법\n",
    "2. 단순선형회귀: 하나의 입력 변수로 출력 예측하기\n",
    "3. 다중선형회귀: 여러 입력 변수를 활용한 예측\n",
    "4. 선형회귀 주의사항: 변수 상관(다중공선성), 상관과 인과 구분\n",
    "\n",
    "<br>\n",
    "\n",
    "> 학습 목표\n",
    "- 선형회귀의 핵심 개념 이해: 입력-출력의 선형 관계, 단순선형회귀 이해\n",
    "- 모델 적합: 잔차제곱합(RSS)과 최소 제공 이해\n",
    "- 다중선형회귀로 확장한 학습 방법 이해"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b113e16",
   "metadata": {},
   "source": [
    "## 0. 학습 시작(Overview)\n",
    "> 실제 응용 사례\n",
    "- 광고비와 매출 관계 분석\n",
    "- 고객 소득·소비 패턴을 활용한 신용점수 산출\n",
    "\n",
    "![img](./img/l1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0b1445",
   "metadata": {},
   "source": [
    "## 1. 선형 회귀\n",
    "### 1-1. 선형회귀(Linear Regression)\n",
    "> 선형회귀란?\n",
    "- 입력 변수와 출력 변수 사이의 관계를 직선 형태로 근사하여, 예측하는 통계적 방법이다.\n",
    "- 지도학습의 가장 기초가 되는 접근 중 하나이다.\n",
    "- 단순해 보이지만, 선형회귀는 개념적으로도, 실무적으로도 매우 유용하다.\n",
    "\n",
    "### 1-2. 광고 데이터 예시\n",
    "> 선형회귀를 통해 대답할 수 있는 질문들\n",
    "- 광고비와 매출 사이에 관계가 있는가?\n",
    "- 그 관계의 강도는 어느 정도인가?\n",
    "- 어떤 매체가 매출에 기여하는가??\n",
    "- 미래 매출을 얼마나 정확히 예측할 수 있는가?\n",
    "- 매체 간에 상호작용(시너지)가 있는가?\n",
    "\n",
    "![img](./img/l2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61567abd",
   "metadata": {},
   "source": [
    "## 2. 단순선형회귀(Simple Linear Regression)\n",
    "### 2-2. 단순선형회귀: 단일 설명변수를 이용한 선형회귀\n",
    "> 단순선형회귀(simple linear regression)란?\n",
    "- 한 개의 설명변수(X)와 하나의 반응변수(Y) 사이의 선형(직선) 관계를 찾는 방법\n",
    "- 목표: 데이터를 가장 잘 설명하는 직선을 찾아 예측($\\hat y$)에 활용\n",
    "\n",
    "- 단일 설명변수를 이용한 단순선형회귀\n",
    "    - 모형 가정: $Y = {\\beta}_0 + {\\beta}_{1}X - \\varepsilon$\n",
    "        - ${\\beta}_0$: 절편(X=0일 때 Y값)\n",
    "        - ${\\beta}_1$: 기울기(X가 1 단위 증가할 때 Y의 평균 증가량)\n",
    "        - $\\varepsilon$: 관측 오차\n",
    "    - hat(예, ${\\hat y}, {\\hat \\beta}$) 표기는 추정값을 의미\n",
    "\n",
    "![image](./img/l3.png)\n",
    "\n",
    "### 2-2. 최소제곱법(least squares)\n",
    "> 최소제곱법(least squares)이란?\n",
    "- 실제 관측값과 예측값의 차이(잔차, residual)를 제곱해 합한 값(RSS, 잔차제곱합)을 최소화하는 방법\n",
    "- 목표: 데이터를 가장 잘 설명하는 직선을 찾기 위해 계수 ${\\beta}_0, {\\beta}_1$을 추정\n",
    "\n",
    "- 잔차(residual)의 정의: $e_i = y_i - \\hat y_i$ (예측값 $\\hat y_i = \\hat \\beta _0 + \\hat \\beta _1 x_i$)\n",
    "- RSS(잔차제곱합) 정의: RSS=$e_1^2 + e_2^2 + ... + e_n^2$\n",
    "- 다른 표현 $RSS=\\sum_{i=1}^n e_i^2 = \\sum_{i=1}^n (y_i - \\hat y_i)^2 = \\sum_{i=1}^n (y_i - \\hat \\beta _0 - \\hat \\beta _1 x_i)^2$\n",
    "\n",
    "- 계수를 측정하기 위한 공식: closed-form solution(공식으로 바로 계산할 수 있는 해) 존재함\n",
    "    <br> 기울기   $\\hat \\beta _1 = \\frac {\\sum ^n _{i=1}(x_i - \\bar x)(y_i-\\bar y)}{\\sum ^n_{i=1}(x_i-\\bar x)^2}$\n",
    "    <br> 절편   $\\hat \\beta _0 = \\bar y - \\hat \\beta _1 \\bar x$\n",
    "    <br> 참고: $\\bar y = \\frac{1}{n} \\sum _i y_i$,  $\\bar x = \\frac {1}{n} \\sum _i x_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2c818c",
   "metadata": {},
   "source": [
    "### 2-3. 단순선형회귀: 광고 데이터\n",
    "> 사례 설명\n",
    "- 목표: TV 광고비(X)과 제품 판매량(Y)의 선형 관계 예측\n",
    "- 단순선형회귀를 적용하여, 각 데이터에서 잔차제곱을 가장 작게 만드는 직선(최소제곱법)이 선택됨\n",
    "- 도형의 의미\n",
    "    - <span style='color: #3399FF'>파란 직선</span>:  최소제곱법으로 계산한 회귀선\n",
    "    - <span style=\"color: #FF6666\">빨간 점</span>: 실제 관측 데이터\n",
    "    - <span style=\"color: #999999\">회색 세로선</span>(빨간 점에서 파란 선까지): 잔차(residual)\n",
    "    - RSS(잔차제곱합)를 최소화할 때, 최적의 $\\hat \\beta _0, \\hat \\beta _1$이 결정됨\n",
    "- 수식\n",
    "<br> 기울기 $\\hat \\beta _1 =\\frac {\\sum ^n _{i=1}(x_i - \\bar x)(y_i-\\bar y)}{\\sum ^n_{i=1}(x_i-\\bar x)^2}$\n",
    "<br> 절편 $\\hat \\beta _0 = \\bar y - \\hat \\beta _1 \\bar x$\n",
    "\n",
    "![image](./img/l4.png)\n",
    "\n",
    "> 단순선형회귀 결과 해석(광고 데이터)\n",
    "- 계수 해석\n",
    "    - 절편(Intercept) = 7.03 → TV 광고비가 0이어도 기본적으로 평균 판매량은 7.03백만원\n",
    "    - TV 광고비 계수 = 0.0475 → TV 광고비를 1단워(1백만원) 늘리면 평균 매출이 약 0.0475x1단위(1백만원) = 4.72만원 증가\n",
    "\n",
    "![image](./img/l5.png)\n",
    "\n",
    "- 유의성 검정\n",
    "    - 계수의 p-value < 0.0001 (매우 작음, << 0.05)이므로 통계적으로 매우 유의함 → <span style=\"color: #3399FF\">TV 광고비와 매출 간 관계 존재</span>\n",
    "    - 모형 적합도($R^2$이 높을수록, 1에 가까울수록 좋음)\n",
    "        - $R^2$=0.612 → 판매량 변동의 약 61%를 광고비로 설명 가능\n",
    "\n",
    "    ![image](./img/l6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636f8c6d",
   "metadata": {},
   "source": [
    "## 3. 다중선형회귀(Multiple Linear Regression)\n",
    "### 3-1. 다중선형회귀(multiple linear regression)란?\n",
    "> 단순선형회귀와 다중선형회귀\n",
    "- 단순 선형 회귀: \"TV 광고 → 매출\" 한 가지 관계만 고려\n",
    "- 다중 선형 회귀: \"TV 광고, Radio 광고비, 가격, 계절, 경쟁사\" 등 복수 요인을 함께 고려하여 매출을 설명\n",
    "\n",
    "![image](./img/l7.png)\n",
    "\n",
    "> 다중선형회귀의 개념\n",
    "- 독립 변수(설명 변수, Feature)가 여러 개 존재할 때 사용하는 회귀 분석 기법\n",
    "- 단순 선형 회귀는 하나의 변수만 고려하지만, 다중 선형 회귀는 여러 독립변수($X_1, X_2, ...$)를 동시에 고려하여, 종속 변수(Y)와의 관계를 구함\n",
    "<br>$Y = \\beta _0 + \\beta _1 X_1 + \\beta _2 X_2 + ... + \\beta _p X_p + \\varepsilon$\n",
    "\n",
    "> 각 변수의 의미\n",
    "- Y: 종속 변수(예측 대상, 예: 매출)\n",
    "- $X_1, X_2, X_3, ..., X_p$: p개의 독립 변수들(예: 광고비, 가격, 계절 등)\n",
    "- $\\beta _0$: 절편\n",
    "- $\\beta _1, \\beta _2, \\beta _3, ..., \\beta _p$: 각 독립 변수에 대한 회귀 계수(모수) <span style=\"color: #3399FF\">(변수의 영향력 크기와 방향을 나타냄)</span>\n",
    "- $\\varepsilon$: 관측 오차(모델이 설명하지 못하는 부분, 오류/잔차가 아님!)\n",
    "    - 해석: 다른 변수를 고정한 채 $X _j$가 1 단위 증가할 때 Y가 평균적으로 $\\beta _j$만큼 변화\n",
    "    - 광고 데이터 예: sales = $\\beta _0$ + $\\beta _1$ TV + $\\beta _2$ radio +  $\\beta _3$ newspaper\n",
    "\n",
    "### 3-2. 다중선형회귀의 추정과 예측\n",
    "> 다중선형회귀: 계수 추정과 예측\n",
    "- 여러 변수($x_{i1}, x_{i2}, ..., x_{ip}$)로 반응변수 Y를 동시에 예측하는 모형\n",
    "- 예측값: $\\hat y_i = \\hat \\beta _0 + \\hat \\beta _1 x_{i1} + \\hat \\beta _2 x_{i2} + ... + \\hat \\beta _p x_{ip}$ 가  최적 추정치\n",
    "- 추정 방법: 실제 값과 예측 값의 차이(잔차, $e_i = y_i - \\hat y_i$)를 제곱해 합한 값(RSS)를 최소화\n",
    "- RSS가 최소일 때 얻어지는 계수 $\\hat \\beta _0, \\hat \\beta _1, ..., \\hat \\beta _p$ 가 최적 추정치\n",
    "<br> $RSS = \\sum \\limits ^n_{i=1} e^2_i = \\sum \\limits ^n_{i=1} (y_i - \\hat y_i)^2 = \\sum \\limits ^n_{i=1} (y_i - \\hat \\beta _0 - \\hat \\beta _1 x_{i1} - ... - \\hat \\beta _p x_{ip})^2$\n",
    "<br>\n",
    "<br> <span style='color: #3399FF'>여러 입력 변수를 동시에 고려하여 데이터와 가장 가까운 평면(hyperplane)을 찾는 과정</span>\n",
    "\n",
    "### 3-3.다중선형회귀 계수 추정 유도(행렬 표현)\n",
    "> 다중선형회귀: 계수 추정의 수학적 유도\n",
    "- 행렬 표현\n",
    "\n",
    "![image](./img/l8.png)\n",
    "\n",
    "- 최소제곱법 목적: RSS 최소화\n",
    "\n",
    "![image](./img/l9.png)\n",
    "\n",
    "-  정규방정식 해\n",
    "\n",
    "![image](./img/l10.png)\n",
    "\n",
    "### 3-4. 다중선형회귀 결과: 시각화\n",
    "![image](./img/l11.png)\n",
    "\n",
    "### 3-5. 다중선형회귀 결과: 광고 데이터\n",
    "> 다중선형회귀 결과 해석 (광고 데이터)\n",
    "- TV, 라디오 광고비는 매출 증가에 유의미한 관계를 가짐(각 p-value들이 매우 낮음 << 0.05)\n",
    "- 신문 광고비는 통계적으로 유의하지 않음(p-value = 0.8599 > 0.05) → 매출과 관계가 거의 없음\n",
    "\n",
    "![image](./img/l12.png)\n",
    "\n",
    "- 모형 적합도($R^2$ 가 높을수록, 1에 가까울수록 좋음)\n",
    "- 결정계수 $R^2$ = 0.897: 모델 설명력이 매우 높음 → 단순선형회귀 결과와 비교하였을 때 향상된 예측\n",
    "\n",
    "![image](./img/l13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da099d9",
   "metadata": {},
   "source": [
    "## 4. 선형회귀 주의사항\n",
    "### 4-1. 검증/테스트셋 데이터를 활용한 성능 평가\n",
    "> 선형회귀 결과 검증 및 테스트 성능\n",
    "- 훈련 데이터에서의 성능\n",
    "    - 회귀식을 만들 때 최소제곱 해는 훈련 데이터만 보고 계산됨\n",
    "    - 학습에 사용된 훈련 데이터에서는(X와 Y의 평균적인 선형관계가 있다면) 적합(fitting)이 잘 되어있을 것임\n",
    "    - 그러나 이것은 테스트 성능을 과소평가할 가능성이 높음\n",
    "- 테스트 성능 평가 필요\n",
    "    - 선형회귀도 변일반화 성능을 확인하려면 훈련에 사용되지 않은 새로운(테스트) 데이터에 적용해봐야 함\n",
    "    - 수가 많거나 고차항을 사용하면 <b>과적합(overfitting) 문제</b>가 여전히 발생할 수 있음\n",
    "    - 검증/교차검증을 통해서 적절한 적합을 찾을 수 있음\n",
    "> 선형회귀를 통해 대답할 수 있는 질문들\n",
    "- 이상적 상황: 변수들이 연관(correlation)되지 않고, 독립적일 때 → 계수 해석이 명확함\n",
    "- 문제 상황: 변수들이 서로 연관되어 있다면 → 계수 추정이 불안정해지고 해석에 혼동이 발생할 수 있음\n",
    "- 주의: 관찰 데이터의 상관관계로 인과 관계를 주장해서는 안됨\n",
    "    - 예로 들었던, 광고 데이터는 자연스럽게 인과성이 있는 것처럼 보이지만, 다른 많은 데이터에서 선형 관계를 보인다고 하여 인과 관계가 있는 것이 아님\n",
    "    - 예: \"아이스크림 소비량\"(X) vs \"상어에 물리는 사건\"(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3765150",
   "metadata": {},
   "source": [
    "### 강의 정리\n",
    "> 선형회귀 개요\n",
    "- 지도학습의 가장 기초적인 방법\n",
    "- 입력과 출력의 선형 관계를 학습\n",
    "\n",
    "> 단순선형회귀\n",
    "- 모형; $Y = \\beta _0 + \\beta _1 X + \\varepsilon$\n",
    "- 의미: $\\beta _0$ : 절편, $\\beta _1$: 기울기, $\\varepsilon$ : 측정오차\n",
    "- 학습 방법: 최소제곱법 (RSS 최소화)\n",
    "- 사례: TV 광고비와 매출 관계\n",
    "\n",
    "> 다중선형회귀\n",
    "- 모형: $Y = \\beta _0 + \\beta _1 X_1 + \\beta _2 X_2 + ... + \\beta _p X_p + \\varepsilon$\n",
    "- 해석: 다른 변수 고정 시, 특정 $X_j$ 가 1 증가 →  Y는 평균적으로 $\\beta _j$ 변화\n",
    "- 사례: TV + Radio + Newspaper 광고비와 매출\n",
    "\n",
    "> 계수 추정과 예측\n",
    "- RSS 최소화를 통해 계수 추정\n",
    "- 해: 최소제곱 → 정규방정식 사용\n",
    "- 결과는 평면(hyperplane)으로 표현\n",
    "\n",
    "> 선형회귀 주의사항\n",
    "- 훈련 데이터 기반 계산 → 검증/테스트셋 필요\n",
    "- 다중공선성 문제 발생 가능(변수 간 높은 상관성)\n",
    "    - 계수 해석 어려움\n",
    "    - 분산 증사 → 불안정한 추정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647046ce",
   "metadata": {},
   "source": [
    "### 확인 문제\n",
    "\n",
    "![image](./img/l14.png)\n",
    "\n",
    "![image](./img/l15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b93e78",
   "metadata": {},
   "source": [
    "# <span style=\"background-color: #FAFAD8\">AI & 기계학습 방법론 1<span>\n",
    "##### 로지스틱회귀(Logistic Regression)\n",
    "> CONTENTS\n",
    "1. 분류(Classification)\n",
    "    1. 분류란?\n",
    "    2. 예시: 신용카드 연체(Default)\n",
    "    3. 분류 모델에서 선형 회귀의 한계\n",
    "2. 로지스틱 회귀(Logistic Regression)\n",
    "    1. 로지스틱(Logistic) 회귀의 모형식\n",
    "    2. MLe 활용 모수 추정\n",
    "    3. 로지스틱 회귀 결과: 신용카드 연체 데이터\n",
    "<br>\n",
    "\n",
    "> 학습 목표\n",
    "- 분류 모델에서 로지스틱 회귀를 사용하는 이유를 이해합니다.\n",
    "- 로지스틱 회귀 모형을 이해합니다.\n",
    "- 신용카드 연체의 예시를 통해 분류 모델을 적용해봅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b3b681",
   "metadata": {},
   "source": [
    "# 0. 학습 시작(Overview)\n",
    "> 분류란 무엇인가?\n",
    "- 분류 모델의 정의, 분류 모델의 목표\n",
    "\n",
    "> 로지스틱 회귀(Logistic Regression)은 어떻게 작동할까?\n",
    "- 로지스틱 회귀의 모형식, MLE(maximum Likelihood Estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b3b9ef",
   "metadata": {},
   "source": [
    "## 1. 분류(Classification)\n",
    "### 1-1. 분류(Classification)란?\n",
    "> 분류의 정의\n",
    "- 분류: 정해진 범주(카테고리) 중 하나로 지정하는 것\n",
    "- 범주형 변수: 수치의 크고 작음이 아니라 유한한 범주(성별, 혈액형, 지역 등)로 표현하는 변수\n",
    "    - 예: 눈동자 색{black, brown, blue, green}, 이메일 종류 {spam, normal}\n",
    "\n",
    "> 분류 함수의 목표\n",
    "- 입력($X$, 특성 벡터): 눈동자 이미지, 이메일 내용/제목\n",
    "- 출력($Y \\in C$, 범주):\n",
    "    - 예, 눈동자 색 C = {black, brown, blue, green}, 이메일 C = {spam, normal}, 순서 없는 집합\n",
    "- 분류 함수의 목표\n",
    "    - 분류 함수 $f(X)$를 학습하여 입력 $X$ 가 속할 범주(카테고리)를 예측\n",
    "    - 범주의 직접 예측보다 각 범주에 속할 확률 P(Y = k|X)를 추정하는 것이 더 유용할 때가 많음\n",
    "\n",
    "### 1-2. 예시: 신용카드 연체(Default)\n",
    "> 신용카드 사용량 및 소득에 대한 연체 여부 산점도\n",
    "- 신용카드 사용량(Balance)-소득(Income) 산점도에 연체(Default) 여부를 색상(주황색 vs 파란색) 및 부호(+  vs O)로 구분\n",
    "- 신용카드 사용량과 소득은 각각 독립변수, 연체 여부가 종속변수\n",
    "\n",
    "> 신용카드 사용량-소득 산점도 해석\n",
    "- <span style=\"color: orange\">연체자(주황색 +)</span>는 신용카드 사용량이 높은 구간에 집중적으로 분포\n",
    "- <span style=\"color: skyblue\">연체가 없는 사람(파란색 O)</span>은 신용카드 사용량이 낮은 쪽에 주로 분포\n",
    "- 소득은 연체 여부와 뚜렷한 상관이 보이지 않음\n",
    "\n",
    "![image](./img/l16.png)\n",
    "\n",
    "> 그룹별 분포(Boxplot)\n",
    "- 신용카드 사용량(Balance), 소득(Income)에 대해 연체(Default)의 그룹별 분포(Boxplot)\n",
    "- 신용카드 사용량과 같은 소득은 각각 독립변수, 연체 여부가 종속변수\n",
    "\n",
    "> 그룹별 분포 해석\n",
    "- 신용카드 사용량(Balance) vs 연체(Default)\n",
    "    - <span style=\"color: orange\">연체자</span>의 신용카드 사용량이 <span style=\"color: skyblue\">연체하지 않은 사람</span>보다 전반적으로 높음\n",
    "    - 중앙값도 높고, 분포가 퍼져있는 정도도 더 큼\n",
    "- 소득(Income) vs 연체(Default)\n",
    "    - 연체 여부에 따른 소득 차이는 거의 없음\n",
    "    - 중앙값이 약간 다르지만 분포가 대부분 겹침\n",
    "\n",
    "![image](./img/l17.png)\n",
    "\n",
    "### 1-3. 분류 문제에 선형회귀를 써도될까?\n",
    "> 선형회귀는 분류 문제에 사용하기에 부적절함: 이진 분류 문제\n",
    "- 선형회귀는 선형함수를 계산하는 문제로 예측 값이 (Y값 기준) 제한된 값을 갖게 못함\n",
    "- 따라서 선형회귀는 예측 확률이 0보다 작거나 1보다 크게 예측될 수 있어 확률로 쓰기 부적절함\n",
    "    - 예: 응급실 환자 진단 {0: 비응급, 1: 응급}\n",
    "    - 선형회귀는 확률 범위(0~1)를 벗어나는 값을 내놓을 수 있어 문제 발생\n",
    "\n",
    "![image](./img/l18.png)\n",
    "\n",
    "> 다중 범주 분류 문제\n",
    "- 선형회귀는 정수형 코딩(1, 2, 3)에 따라 범주 간 순서와 동일한 거리를 가정\n",
    "- 범주(카테고리) 볒ㄴ수는 순서가 없는 라벨이므로 부적절함\n",
    "    - 예: 응급실 환자 진단 {1: 뇌졸증, 2: 약물과다복용, 3: 간질발작}\n",
    "    - 실제로는 범주 간 순서나 거리가 존재하지 않음\n",
    "- 따라서, 선형회귀는 분류 문제에 부적절함\n",
    "\n",
    "![image](./img/l19.png)\n",
    "\n",
    "### 1-4. 분류 문제에서 적합한 모델\n",
    "> 분류 문제에서 선형회귀의 대안 - 로지스틱 회귀(Logistic Regression)\n",
    "- 시그모이드(Sigmoid) 함수를 활용해 0~1 범위 내 확률값 예측 보장\n",
    "- 순서가 없는 범주를 확률로 직접 예측하는 적절한 분류 방법\n",
    "\n",
    "![image](./img/l20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2757a11",
   "metadata": {},
   "source": [
    "## 2. 로지스틱회귀 (Logistic Regression)\n",
    "### 2-1. 로지스틱(Logistic)회귀의 모형식\n",
    "> 이진 분류에서의 적절한 함수\n",
    "- 함수의 출력 범위가 모든 입력에 대해 0~1 ㅍ 사이의 범위를 가지는 함수를 활용하자\n",
    "\n",
    "> 출력 범위가 0~1인 시그모이드(Sigmoid) 함수\n",
    "- 기울어진 S자 형태의 곡선 함수\n",
    "    <br>$y = \\frac {e^z}{1+e^z}$ = $\\frac {1}{1+e^{-z}}$\n",
    "    <br> (오일러 수 e $\\approx$ 2.71828)\n",
    "- 모든 실수 z에 대한 y값의 범위는 $0 \\leq y \\leq 1$이다.\n",
    "    <br> $z → + \\infty$일 때, y는 1로 수렴\n",
    "    <br> $z → - \\infty$일 때, y는 0로 수렴\n",
    "\n",
    "![image](./img/l21.png)\n",
    "\n",
    "> 로지스틱 회귀(Logistic Regression)의 모형식\n",
    "- 시그모이드(Sigmoid) 함수 $y = \\frac{e^z}{1+e^z}$ 의 z에 (Linear Regression) 식 z = $\\beta -0 + \\beta _1 x$ 식을 대입하면, $y = \\frac{e^{\\beta _0 + \\beta _1 x}}{1 + e^{\\beta _0 + \\beta _1 x}}$\n",
    "\n",
    "- 확률 표기 $p(x)$를 활용한다면, 로지스틱 회귀의 모형식이 된다.\n",
    "![image](./img/l22.png)\n",
    "![image](./img/l23.png)\n",
    "\n",
    "### 2-2. 로지스틱 회귀 모형 vs 선형 모형\n",
    "> 로지스틱 함수와 선형회귀의 관계\n",
    "- 로지스틱 함수 모형식 $p(y=1|x) = p(x; \\beta) = \\frac {e^{\\beta _0+\\beta _1}}{1+e^{\\beta _0+\\beta _1 x}}$ 에 로짓 변환을 수행해보자\n",
    "<br> $logit(p) = log(odds) = log \\frac{p(y=1|x)}{1-p(y=1|x)} = log \\frac{p(x; \\beta)}{1-p(x; \\beta)} = \\beta _0 + \\beta _1 x$\n",
    "- 즉, 로지스틱 모형식은 선형 모형식과 시그모이드(sigmoid) 함수의 결합이며, 로짓 변환 시 선형 회귀 모형식으로 표현이 가능하다.\n",
    "\n",
    "![img](./img/l24.png)\n",
    "\n",
    "### 2-3. MLE 활용 모수 추정\n",
    "> 우도(Likelihood)\n",
    "- 확률을 추정하는 모델을 결정했으니, 모수($\\beta _0, \\beta _1$)를 추정하는 방법에 대해 알아보자.\n",
    "- 선형회귀에서 \"현재 함수가 데이터와 오차가 작은지\"를 평가하기 위해 평균 제곱 오차(MSE:Mean Squered Error)를 지표로 삼았 듯, 확률을 계산하는 함수를 평가하기 위해선 우도(Likelihood)를 지표로 삼는다.\n",
    "- 우도란 \"현재 확률 함수가 데이터를 얼마나 잘 설명하는지\"를 나타낸 지표이다.\n",
    "    <br>그러므로, 모델의 학습은 우도 값을 높여 최대화가 되도록 하는 것이 목표이며 이를 Maximum Likelihood Estimation(MLE)이라 한다.\n",
    "- 로지스틱 회귀의 우도 최대화\n",
    "- 로지스틱 회귀와 같은 이진 분류 문제에서 우도를 최대화한다.\n",
    "<br> ![img](./img/l25.png)\n",
    "- 하지만, 위와 같은 곱으로 이뤄진 함수의 경우, $\\beta$에 대해 미분이 어렵기 때문에 양변에 log를 취해 곱셈을 더하기로 변환시킨 뒤 log-likelihood를 만들어 최대화한다.\n",
    "<br> ![img](./img/l26.png)\n",
    "- log $\\mathcal{L} (\\beta)$를 미분하여 도함수=0에 근접하도록 수치적(반복) 최적화를 통해 $\\beta$들을 찾아\n",
    "나간다.\n",
    "\n",
    "> log-likelihood로 변형하여 최대화하는 $\\beta$를 구해도 되는가?\n",
    "- 로지스틱 회귀에서 우도(likelihood)를 최대화하는 것인데, log변환을 한 log-likelihood를 최대화하는 것으로 변형하여 문제를 접근해도 과연 괜찮을까?\n",
    "- Log 함수는 단조(monotone) 증가 함수임\n",
    "- 따라서, log-likelihood를 최대화하는 $\\beta$와 likelihood를 최대화하는 $\\beta$는 같다.\n",
    "\n",
    "![img](./img/l27.png)\n",
    "\n",
    "### 2-4. 로지스틱 회귀 결과: 신용카드 연체 데이터\n",
    "> 신용카드 연체 데이터 결과 해석\n",
    "- 신용카드 사용량(Balance)과 연체 여부(Default)로 로지스틱 회귀 모형을 학습한 결과\n",
    "\n",
    "![img](./img/l28.png)\n",
    "\n",
    "- 추정 결과 $\\hat \\beta _1 = 0.0055$\n",
    "<br>즉, 신용카드 사용량(Balance)이 1 단위 증가할 때 연체(Default)의 로짓(log-odds)가 0.0055 증가\n",
    "<br>$log \\frac{p(X;\\hat \\beta)}{1-p(X;\\hat \\beta)} = -10.6513 + 0.0055 {\\cdot} X$\n",
    "\n",
    "![img](./img/l29.png)\n",
    "\n",
    "- 신용카드 사용량 이외에도 다양한 입력(소득, 학생 여부 등)을 추가하여 여러 변수 $X_1, ..., X_p$를 함께 모형에 사용하면 회귀 계수를 통해 연체할 확률을 계산할 수 있음\n",
    "\n",
    "![img](./img/l30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e194bebe",
   "metadata": {},
   "source": [
    "### 강의 정리\n",
    "> 분류 문제란?\n",
    "- 분류: 정해진 범주(카테고리) 중 하나로 지정하는 것\n",
    "- 분류 모델의 목표: 분류 함수 f(X)를 학습하여 입력 X가 속할 범주(또는 그 확률)를 예측\n",
    "\n",
    "> 분류 문제에서 선형회귀 모델 적용의 한계\n",
    "- 이진 분류 문제: 선형회귀는 예측값이 확률 범위(0~1)를 벗어나 부적합\n",
    "- 다중 분류 문제: 정수로 코딩 시 인위적인 순서를 암시하기에 부적합\n",
    "\n",
    "> 로지스틱 회귀(Logistic Regression)는 어떻게 작동할까?\n",
    "- 로지스틱 함수 모형식은 선형 회귀 모형식과 sigmoid 함수의 결합\n",
    "- 우도: \"확률함수가 데이터를 얼마나 잘 설명하는지\"를 나타낸 지표\n",
    "- 최대 우도 추정(MLE: Maximum Likelihood Estimation): 우도가 최대가 되는 모수를 찾는 과정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ea0905",
   "metadata": {},
   "source": [
    "### 확인 문제\n",
    "![img](./img/l31.png)\n",
    "\n",
    "![img](./img/l32.png)\n",
    "\n",
    "![img](./img/l33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b819eff",
   "metadata": {},
   "source": [
    "# <span style=\"background-color: #FAFAD8\">AI & 기계학습 방법론 1<span>\n",
    "##### 신경망모델(Neural Network)\n",
    "> CONTENTS\n",
    "1. Shallow 네트워크\n",
    "  1. 구조 및 활성화 함수\n",
    "  2. 조각별 선형(piecewise linear) 표현\n",
    "  3. 표현력/보편적 근사 정리\n",
    "2. Deep Network\n",
    "  1. 네트워크 합성과 층별 출력, \"접기(folding)\" 직관\n",
    "  2. Shallow vs Deep 비교\n",
    "  3. 행렬 $\\cdot$ 벡터 수식화와 도식\n",
    "\n",
    "<br>\n",
    "\n",
    "> 학습 목표\n",
    "- 신경망이란 무엇인지를 알아보고, 기본 구성요소를 설명한다.\n",
    "- Hidden unit의 역할(비선형성 도입$\\cdot$특징 변환)을 이해한다.\n",
    "- Shallow 네트워크와 Deep 네트워크의 차이(구조, 표현력, 효율성)를 구분한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d2d359",
   "metadata": {},
   "source": [
    "## 0. 학습 시작(Overview)\n",
    "> 신경망(Neural Network)이란 무엇인가?\n",
    "- Shallow 네트워크, Deep 네트워크의 구조\n",
    "\n",
    "> 네트워크의 hidden unit은 어떤 역할을 하는가?\n",
    "- Shallow 네트워크, Deep 네트워크에서 hidden unit 및 활성화의 역할\n",
    "\n",
    "> Shallow 네트워크와 Deep 네트워크의 차이점은 무엇인가?\n",
    "- Deep 네트워크의 표현력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e3dc1c",
   "metadata": {},
   "source": [
    "## 1. 모수적 함수로서의 선형 모델\n",
    "### 1-1. 단순(1D) 선형모델: 모수적 함수\n",
    "\n",
    "![img](./img/l34.png)\n",
    "\n",
    "### 1-2. 단순(1D) 선형모델 학습\n",
    "\n",
    "![img](./img/l35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68a6c1c",
   "metadata": {},
   "source": [
    "## 2. Shallow 네트워크\n",
    "### 2-1. Shallow 네트워크 vs 1D 선형회귀\n",
    "\n",
    "![img](./img/l36.png)\n",
    "\n",
    "### 2-2. Shallow 네트워크: 활성화 함수\n",
    "\n",
    "![img](./img/l37.png)\n",
    "\n",
    "### 2-3. Shallow 네트워크: 모수(parameter)\n",
    "\n",
    "![img](./img/l38.png)\n",
    "\n",
    "### 2-4. Shallow 네트워크: piecewise linear 함수\n",
    "\n",
    "![img](./img/l39.png)\n",
    "\n",
    "### 2-5. Shallow 네트워크: Hidden Units\n",
    "\n",
    "![img](./img/l40.png)\n",
    "\n",
    "### 2-6. Shallow 네트워크: 각 단계별 계산\n",
    "\n",
    "![img](./img/l41.png)\n",
    "![img](./img/l42.png)\n",
    "![img](./img/l43.png)\n",
    "\n",
    "### 2-7. 네트워크 도식화\n",
    "\n",
    "![img](./img/l44.png)\n",
    "![img](./img/l45.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc7c4b4",
   "metadata": {},
   "source": [
    "## 3. Shallow 네트워크의 표현력\n",
    "### 3-1. 더 많은 Hidden Unit 기능\n",
    "\n",
    "![img](./img/l46.png)\n",
    "\n",
    "### 3-2. Hidden Unit을 많이 두면\n",
    "<br> 충분히 많은 Hidden Unit이 있다면, 임의의 1차원 함수를 원하는 정확도로 근사할 수 있다.\n",
    "<br> \"Hidden unit을 충분히 많이 갖는다면, 얕은 신경망은 임의의 연속함수를 임의의 정밀도로 근사할 수 있음.\"\n",
    "\n",
    "![img](./img/l47.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317f984b",
   "metadata": {},
   "source": [
    "## 4. 다중 출력/입력\n",
    "### 4-1. 2개 출력의 네트워크\n",
    "<br>1 input, 4 hidden units, 2 outputs\n",
    "\n",
    "![img](./img/l48.png)\n",
    "\n",
    "### 4-2. 2개 입력의 네트워크\n",
    "<br>2 input, 3 hidden units, 1 outputs\n",
    "\n",
    "![img](./img/l49.png)\n",
    "\n",
    "### 4-3. 2개 입력의 네트워크: 단계별 계산\n",
    "\n",
    "![img](./img/l50.png)\n",
    "![img](./img/l51.png)\n",
    "![img](./img/l52.png)\n",
    "\n",
    "### 4-4. 2개 입력: From Input to Output\n",
    "\n",
    "![img](./img/l53.png)\n",
    "\n",
    "### 4-5. 임의의 개수의 입력, Hidden Unit, 출력\n",
    "\n",
    "![img](./img/l54.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551385f5",
   "metadata": {},
   "source": [
    "## 5. Deep 네트워크\n",
    "### 5-1. 2개의 네트워크를 하나로 합성\n",
    "\n",
    "![image](./img/l55.png)\n",
    "\n",
    "### 5-2. 합성 네트워크의 층(layer)별 출력\n",
    "\n",
    "![image](./img/l56.png)<br>\n",
    "![image](./img/l57.png)<br>\n",
    "![image](./img/l58.png)<br>\n",
    "![image](./img/l59.png)<br>\n",
    "![image](./img/l60.png)<br>\n",
    "![image](./img/l61.png)<br>\n",
    "![image](./img/l62.png)<br>\n",
    "![image](./img/l63.png)<br>\n",
    "![image](./img/l64.png)<br>\n",
    "![image](./img/l65.png)<br>\n",
    "![image](./img/l66.png)<br>\n",
    "![image](./img/l67.png)<br>\n",
    "![image](./img/l68.png)<br>\n",
    "![image](./img/l69.png)<br>\n",
    "![image](./img/l70.png)<br>\n",
    "![image](./img/l71.png)<br>\n",
    "![image](./img/l72.png)<br>\n",
    "![image](./img/l73.png)<br>\n",
    "![image](./img/l74.png)<br>\n",
    "![image](./img/l75.png)<br>\n",
    "\n",
    "### 5-3. \"접기\" 비유\n",
    "\n",
    "![image](./img/l76.png)\n",
    "\n",
    "### 5-4. Shallow 네트워크와 Deep 네트워크 비교\n",
    "\n",
    "![image](./img/l77.png)\n",
    "\n",
    "### 5-5. 2D Input: 2개의 네트워크를 하나로 합성\n",
    "\n",
    "![image](./img/l78.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de1dfa0",
   "metadata": {},
   "source": [
    "## 6. Deep 네트워크 수식 표현\n",
    "### 6-1. 2개의 네트워크를 하나로 합성: 복잡한 수식\n",
    "\n",
    "![img](./img/l79.png)\n",
    "\n",
    "### 6-2. 2개의 네트워크를 하나로 합성: 새로운 변수 활용\n",
    "\n",
    "![img](./img/l80.png)\n",
    "\n",
    "### 6-3. 2층 네트워크 표현\n",
    "\n",
    "![img](./img/l81.png)\n",
    "![img](./img/l82.png)\n",
    "![img](./img/l83.png)\n",
    "\n",
    "\n",
    "### 6-4. 2층 네트워크: 단계별 계산\n",
    "\n",
    "![img](./img/l84.png)\n",
    "![img](./img/l85.png)\n",
    "\n",
    "### 6-5. 용어의 단순화\n",
    "\n",
    "![img](./img/l86.png)\n",
    "![img](./img/l87.png)\n",
    "![img](./img/l88.png)\n",
    "![img](./img/l89.png)\n",
    "\n",
    "### 6-6. 네트워크 도식화\n",
    "![image](./img/l90.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635b9fb2",
   "metadata": {},
   "source": [
    "# <span style=\"background-color: #FAFAD8\">AI & 기계학습 방법론 4</span>\n",
    "##### 신경망 적합(Fitting)\n",
    "\n",
    "<br>\n",
    "\n",
    "## 0. 학습 시작(Overview)\n",
    "> 손실함수란 무엇일까?\n",
    "- 모델이 예측을 얼마나 잘못했는지 측정하는 함수\n",
    "\n",
    "> 학습이란 무엇일까?\n",
    "- 손실함수를 최소화하는 파라미터(모델의 가중치)를 찾는 과정\n",
    "\n",
    "> 경사 하강법이란 무엇일까?\n",
    "- 손실함수를 줄이기 위해 기울기를 따라 내려가는 방법\n",
    "\n",
    "> 확률적 경사 하강법(SGD)의 장점은 무엇일까?\n",
    "- 전체 데이터 대신 일부만 사용하여 업데이트 별 계산량이 적은 반면 local 최소점에 빠질 확률이 적은 최적화 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b6bed7",
   "metadata": {},
   "source": [
    "## 1. 선형회귀 예시\n",
    "### 1-1. 손실함수\n",
    "> 손실함수\n",
    "- 학습 데이터셋: input/output의 I개 쌍\n",
    "  - 학습 데이터셋 표기: $\\left \\{ X_i, y_i \\right \\}_{i=1}^I$\n",
    "- 손실함수(Loss function): 모델이 얼마나 잘못 예측하는지를 측정하는 함수\n",
    "  - 값이 작을수록 모델이 더 정확하게 학습되었다는 의미\n",
    "\n",
    "  ![img](./img/l91.png)\n",
    "\n",
    "### 1-2. 학습\n",
    "> 손실함수\n",
    "- 학습: 손실함수를 최소화하는 파라미터를 찾음\n",
    "\n",
    "  ![img](./img/l92.png)\n",
    "\n",
    "### 1-3. 1D 선형회귀 예\n",
    "> 1D 선형회귀 예\n",
    "- 선형회귀 모델에서 손실함수는 예측값과 실제값의 차이를 제곱하여 합산한 값(최소제곱 손실함수, MSE)\n",
    "- 학습의 목적은 이 손실함수를 최소화하는 직선을 찾는 것\n",
    "- 손실함수 정의(최소 제곱 손실함수)\n",
    "\n",
    "![img](./img/l93.png)\n",
    "\n",
    "### 1-4. 1D 선형회귀 학습\n",
    "> 선형회귀 학습\n",
    "- 등고선(아래 그림): 손실 함수 값의 크기\n",
    "  - 밝을수록 손실이 큼\n",
    "  - 어두울수록 손실이 적음\n",
    "\n",
    "- 데이터와 선형함수 직선(초록색 선): 데이터와 선의 오차가 크다면 손실값이 큼\n",
    "  - 주황색 점: 실제 데이터\n",
    "  - 초록색 직선: 현재 파라미터로 만든 모델\n",
    "\n",
    "> 경사 하강법(gradient descent)\n",
    "- 손실함수의 값이 줄어드는 방향으로 파라미터를 이동하는 과정\n",
    "\n",
    "![img](./img/l94.png)\n",
    "![img](./img/l95.png)\n",
    "![img](./img/l96.png)\n",
    "![img](./img/l97.png)\n",
    "![img](./img/l98.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f1c4f",
   "metadata": {},
   "source": [
    "## 2. 수학 리뷰\n",
    "### 2-1. 미분을 이용한 최적화\n",
    "> 미분을 통한 기울기 이해\n",
    "- 이차 함수로, 그래프는 위로 열린 포물선\n",
    "- 미분 결과(2x-4)는 기울기를 의미\n",
    "  - 기울기가 0이 되는 지점이 극값(최솟값)\n",
    "\n",
    "![img](./img/l99.png)\n",
    "![img](./img/l100.png)\n",
    "![img](./img/l101.png)\n",
    "![img](./img/l102.png)\n",
    "![img](./img/l103.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82823cf",
   "metadata": {},
   "source": [
    "## 3. 경사하강(Gradient Descent)\n",
    "### 3-1. 경사 하강법\n",
    "> 경사 하강(Gradient descent) 알고리즘\n",
    "- 경사 하강법은 손실 함수 $L[\\phi]$를 최소화하기 위해 파라미터 $\\phi$를 반복적으로 갱신하는 알고리즘\n",
    "- 기울기 계산; 손실함수 $L[\\phi]$를 파라미터 $\\phi$에 대해 편미분(각 원소에 대한 미분) 진행\n",
    "  - 벡터 형태의 기울기\n",
    "\n",
    "  ![img](./img/l104.png)\n",
    "\n",
    "- 파라미터 업데이트\n",
    "  - 기울기(미분값)의 반대 방향으로 이동해야 손실 함수가 줄어듦\n",
    "  <br> $\\phi \\longleftarrow \\phi - \\alpha \\frac{\\partial L}{\\partial \\phi}$\n",
    "\n",
    "  - 여기서 $a \\geq 0$는 학습률(learning rate)로, 한 번의 스텝에서 이동하는 크기를 결정\n",
    "> 손실 함수의 기울기 계산\n",
    "- 경사 하강법의 첫 단계: 편미분(존체 손실에 대한 기울기) 구하기\n",
    "  - 손실 함수: 여기서 $\\ell _i$는 각 데이터 샘플 $i$의 손실값\n",
    "  \n",
    "  ![img](./img/l105.png)\n",
    "\n",
    "  - 전체 손실 $L[\\phi]$은 모든 데이터 샘플 손실 $\\ell _i$의 합\n",
    "  - 전체 기울기는 각 데이터의 기울기를 합한 것\n",
    "  \n",
    "  ![img](./img/l106.png)\n",
    "\n",
    "### 3-2. 경사 하강법: 단계별 계산\n",
    "> 경사 하강법의 단계별 계산\n",
    "- 경사 하강법의 첫 단계: 미분값 구하기\n",
    "\n",
    "![img](./img/l108.png)\n",
    "\n",
    "![img](./img/l107.png)\n",
    "\n",
    "- 두번째 단계: 파라미터 업데이트\n",
    "  - 미분값의 방향의 반대 방향으로 이동(손실 최소화)\n",
    "  <br> $\\phi \\longleftarrow \\phi - \\alpha \\frac{\\partial L}{\\partial \\phi}$\n",
    "    <br> a = 학습률(Learning rate)\n",
    "  \n",
    "![img](./img/l109.png)\n",
    "\n",
    "> 경사 하강법을 활용한 파라미터 업데이트\n",
    "\n",
    "![img](./img/l110.png)\n",
    "\n",
    "![img](./img/l111.png)\n",
    "![img](./img/l112.png)\n",
    "\n",
    "> 경사 하강법을 활용한 파라미터 업데이트\n",
    "\n",
    "![img](./img/l113.png)\n",
    "![img](./img/l114.png)\n",
    "![img](./img/l115.png)\n",
    "\n",
    "> 3-3 한눈으로 보는 경사 하강법\n",
    "> 경사 하강법의 과정\n",
    "\n",
    "a) 데이터 <br>\n",
    "b) 손실 함수 3D 그래프 <br>\n",
    "c) 손실 함수 등고선(2D) <br>\n",
    "d) 선형함수의 업데이트 <br>\n",
    "\n",
    "![img](./img/l116.png)\n",
    "\n",
    "### 3-4. 함수에 따른 최적화 난이도\n",
    "> Convex vs Non-convex 최적화 문제\n",
    "- Convex: 곡선이 항상 U자터럼 아래로 볼록. 그래프 위 임의의 두 점을 잇는 직선이 그래프 위(또는 같은 위치)로 있음\n",
    "- Non-convex: 봉우리, 골짜기, 오목한 구간이 섞인 모양. 두 점을 이은 직선이 그래프 아래로 내려가는 구간이 생김\n",
    "- 손실함수 모양에 따라 최적화의 난이도가 달라짐\n",
    "  - Convex 문제: 전역(global) 최솟값이 유일함 → 최적화가 쉬움\n",
    "  - Non-Convex 문제: 여러 개의 지역(local) 최솟값 또는 새들(saddle)점이 있음 → 최적화가 어려움\n",
    "\n",
    "![img](./img/l117.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af332bf",
   "metadata": {},
   "source": [
    "## 4. 확률적 경사 하강법(Stochastic Gradient Descent)\n",
    "### 4-1. 경사 하강법 vs 확률적 경사 하강법\n",
    "> 경사 하강법의 단점\n",
    "- Non-convex 문제에서 지역(local) 최소점에 빠지기 쉬움\n",
    "  - 그림: 점(1, 2, 3)과 경로는 경사 하강법으로 Loss를 줄여가는 과정\n",
    "  - 점 2에서 출발한 경사하강 방법은 local 최소점에 빠짐\n",
    "\n",
    "  - 매 스텝마다 전체 데이터에 대한 미분값을 구하여 업데이트함. 스텝별 계산양이 많음\n",
    "\n",
    "> 대안: 전체 데이터를 한 번에 쓰는 대신, 무작위로 선택한 데이터 샘플 사용\n",
    "<br> 확률적 경사 하강법(Stochastic Gradient Decsent, SGD)\n",
    "\n",
    "### 4-2. 확률적 경사 하강법의 개념\n",
    "> 업데이트 방식의 차이점\n",
    "- 경사 하강법(Gradient Descent)\n",
    "  - 전체 데이터셋을 사용하여 기울기를 계산(미분)하고, 파라미터 업데이트\n",
    "\n",
    "  ![img](./img/l118.png)\n",
    "\n",
    "- 확률적 경사 하강법(SGD Mini-batch 버전)\n",
    "  - 무작위 확률로 샘플된 일부 데이터(batch)만 사용하여 기울기 계산\n",
    "  \n",
    "  ![img](./img/l119.png)\n",
    "\n",
    "### 4-3. 확률적 경사 하강법 결과 예시\n",
    "> 전체 데이터(경사 하강법) vs 확률적 샘플 데이터를 활용한 확률적 경사 하강법 실행 차이\n",
    "\n",
    "  ![img](./img/l120.png)\n",
    "\n",
    "### 4-4. 확률적 경사 하강법의 특성\n",
    "> 확률적 경사 하강법(SGD)의 특성\n",
    "- 무작위 샘플 데이터를 활용한 미분으로 경로의 무작위성이 있음\n",
    "\n",
    "  ![img](./img/l121.png)\n",
    "\n",
    "- Local 최소점에서 빠질 위험이 상대적으로 적음\n",
    "\n",
    "  ![img](./img/l120.png)\n",
    "\n",
    "\n",
    "> 확률적 경사 하강법(SGD)의 특성\n",
    "- 국소 최솟값\n",
    "  - 전체 데이터가 아닌 일부 배치로 기울기를 계산하기 때문에 노이즈가 섞여있음\n",
    "  - 노이즈가 오히려 local minima, saddle point에서 빠져나오는 도움이 됨\n",
    "\n",
    "- 노이즈가 있지만 여전히 타당한 업데이트\n",
    "  - 미니배치의 기울기는 정확한 전체 기울기가 아니지만, 평균적으로 올바른 방향을 가리킴\n",
    "  - 학습이 점진적 최적점 방향으로 수렴\n",
    "\n",
    "- 계산 비용 절감\n",
    "  - 전체 데이터셋에서 작은 배치만 사용하므로 반복(스텝) 당 연산량이 적음\n",
    "  - 큰 데이터셋에서 효율적으로 학습 가능\n",
    "\n",
    "- 수렴 특성\n",
    "  - full batch처럼 매끄럽게 수렴하지 않고, 무작위성 때문에 더 ㅁ낳이 진동(jitter, 지그재그)하면서 움직임\n",
    "  - 전역 최솟값 근처의 좋은 해에 도달할 수 있음. Convex 문제에서는 full-batch 경사하강보다 수렴이 늦을 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68a306f",
   "metadata": {},
   "source": [
    "## 5. 역전파(Backpropagation)\n",
    "### 5-1. 네트워크 파라미터의 미분\n",
    "\n",
    "> 네트워크 파라미터의 미분값을 어떻게 구할까?\n",
    "- Layer 별로 파라미터들이 존재함. 서로 영향을 미침\n",
    "\n",
    "![img](./img/l122.png)\n",
    "![img](./img/l123.png)\n",
    "![img](./img/l124.png)\n",
    "![img](./img/l125.png)\n",
    "\n",
    "- 네트워크는 합성함수임\n",
    "\n",
    "### 5-2. 합성함수의 미분: 연쇄법칙\n",
    "> 연쇄법칙\n",
    "- 파라미터는 $\\Omega$를 갖는 합성함수를 미분하면?\n",
    "<br> $y = f_1 (f_0(x; \\Omega))$\n",
    "\n",
    "- 핵심 아이디어: 합성함수의 변화율 = 바깥의 변화율 x 안쪽의 변화율\n",
    "<br> $\\frac{\\partial y}{\\partial \\Omega} = \\frac{\\partial f_1 (f_0(x; \\Omega))}{\\partial f_0(x; \\Omega)} \\cdot \\frac{\\partial f_0(x; \\Omega)}{\\partial \\Omega}$\n",
    "\n",
    "- 해석:\n",
    "  - $f_1$의 입력($f_0$)에 대한 기울기를 구한 뒤,\n",
    "  - 글 값을 $f_0$의 파라미터 $\\Omega$에 대한 기울기와 곱한다.\n",
    "  - $x$는 파라미터가 아니므로 $\\partial y / \\partial x$는 여기서 필요 없음\n",
    "\n",
    "### 5-3. 역전파(Backpropagation)\n",
    "> 역전파란?\n",
    "- 출력 오차를 기준으로 그래프를 거꾸로 따라가며 연쇄법칙으로 각 노드(파라미터 포함)의 미분값을 계산하는 절차\n",
    "1. 각 단계 별 계산을 분해\n",
    "2. 각 layer 별 값($f_k$)을 계산\n",
    "3. 각 layer 별 값($f_k$)에 대한 출력 손실($\\ell _i$)의 미분을 구함\n",
    "\n",
    "![img](./img/l126.png)<br>\n",
    "![img](./img/l127.png)<br>\n",
    "![img](./img/l128.png)<br>\n",
    "![img](./img/l129.png)<br>\n",
    "![img](./img/l130.png)<br>\n",
    "![img](./img/l131.png)<br>\n",
    "![img](./img/l132.png)\n",
    "\n",
    "4. 파라미터에 대한 미분을 구함\n",
    "\n",
    "![img](./img/l133.png)<br>\n",
    "![img](./img/l134.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1c4347",
   "metadata": {},
   "source": [
    "### 강의 정리\n",
    "> 경사 하강법(Gradient Descent)\n",
    "- Step 1: 손실함수의 기울기(미분값) 계산\n",
    "- Step 2: 기울기의 반대 방향으로 파라미터 업데이트\n",
    "- 학습률($\\alpha$): 한 번에 이동하는 크기 결정\n",
    "\n",
    "> Convex 문제와 최적화\n",
    "- 2차 미분이 항상 양수 → 아래로 볼록한 형태\n",
    "- 국소 최솟값 = 전역 최솟값 → 안정적인 학습 가능\n",
    "\n",
    "> 확률적 경사 하강법 (SGD)\n",
    "- 전체가 아닌 일부(미니배치) 데이터로 기울기 계산 → 계산 비용 감소\n",
    "- 매 업데이트마다 노이즈가 섞여 국소 최솟값에서 벗어나기 유리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121358e5",
   "metadata": {},
   "source": [
    "### 확인문제\n",
    "![img](./img/l135.png)\n",
    "![img](./img/l136.png)\n",
    "![img](./img/l137.png)\n",
    "![img](./img/l138.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db130542",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
