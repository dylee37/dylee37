{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a37f098",
   "metadata": {},
   "source": [
    "# ⭐ 딥러닝 및 이미지 파운데이션 모델\n",
    "##### <이미지 파운데이션 모델>\n",
    "\n",
    "> CONTENTS\n",
    "1. AI 파운데이션 모델 개념 및 대표 모델\n",
    "2. Vision-Language Model (VLM)\n",
    "3. Small VLM과 파운데이션 모델들 소개\n",
    "4. 개인화, 합성 데이터 활용 사례"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b799f1e3",
   "metadata": {},
   "source": [
    "## <span style=\"background-color:#E6E6FA\"> 1차시. AI 파운데이션 모델 개념 및 대표 모델 </span>\n",
    "\n",
    "<br>\n",
    "\n",
    "> 학습 시작\n",
    "- 학습 데이터와 계산 리소스가 부족한 가난한 AI 서비스 회사(개발자)가 멋진 이미지 기반 AI 서비스를 개발하고자 한다. \n",
    "  <br>어디서부터 시작하면 좋을까?\n",
    "- 기존 이미지 AI 모델은 학습된 도메인과 유사한 데이터에서'만' 잘 작동해왔다\n",
    "  <br> 그러나 실제 사용자 서비스는 전혀 예측하지 못한 데이터가 주어지는 경우도 많다.\n",
    "  <br> 학습하지 않은 상황에서도 동작하는 서비스를 개발하고자 할 때, 어떤 특징을 갖는 이미지 AI 모델이 필요할까?\n",
    "- 실용적인 이미지 AI 모델들을 가지고 어떤 서비스를 만들 수 있을까?\n",
    "\n",
    "<br>\n",
    "\n",
    "> 학습 목표\n",
    "- AI 파운데이션 모델의 개념을 이해하고, 특징을 설명할 수 있다.\n",
    "- Pre-training, Fine-tuning, Zero/Few-shot 개념을 구분할 수 있다.\n",
    "- 기존 머신러닝 모델과의 차이를 이해할 수 있다.\n",
    "- 대표적 AI 파운데이션 모델 중 하나인 CLIP 모델의 개념과 역량을 알아본다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b8bae7",
   "metadata": {},
   "source": [
    "### AI 파운데이션 모델의 개념\n",
    "#### 1-1. 파운데이션 모델(Foundation model)이란?\n",
    "> AI 모델\n",
    "- 함수 또는 프로그램\n",
    "- 입출력을 연결해주는 함수 + 데이터로 학습된 함수 + 학습 때 보지 못했던 데이터에 대해서도 작동해야하는 의무\n",
    "- 예시: 뉴럴넷 | 입력 → 뉴럴넷 → 출력\n",
    "\n",
    "![image](./img/fm1.png)\n",
    "\n",
    "> 이상적인 AI 모델\n",
    "- 만약 AI 모델이 이 세상에서 발생 가능한 [모든 데이터]와 [각 데이터의 설명]을 모두 기억하고 있다면?\n",
    "- 내가 얻고싶은 답과 유사한 답이 이미 DB에 저장되어 있을 확률이 높음 → 검색 엔진과 유사\n",
    "- 예시: 최근접 이웃 탐색 (Nearest Neighbor Search) 알고리즘\n",
    "- 그러나, 데이터 확보, 저장, 탐색은 매우 비용이 크고 현실적이지 않음\n",
    "\n",
    "![image](./img/fm2.png)\n",
    "\n",
    "> 현실적인 기계학습 모델\n",
    "- 학습 = AI 모델에 데이터를 패턴화하여 압축\n",
    "- 이 과정에서 비슷함과 다름을 파악하게 되고, 패턴을 익히면서 새로운 데이터에 대한 일반화 능력이 생김\n",
    "\n",
    "![image](./img/fm3.png)\n",
    "\n",
    "> 파운데이션 모델이란?\n",
    "- 대규모 데이터를 폭넓게 학습한 후, 다양한 문제에 빠르게 적응할 수 있는 범용 대형 AI 모델\n",
    "- 미국 스탠포드 대학 사람 중심 AI 연구소에서 2021년 출간된 보고서[1]에서 새로운 범주로 구분을 시작\n",
    "\n",
    "![image](./img/fm4.png)\n",
    "\n",
    "> 파운데이션 모델\n",
    "- 기존 딥러닝 개발 패러다임: 아기와 같이 언어, 시각, 청각, 촉각 등 기본적인 것들부터 배워 나가야 함\n",
    "- 파운데이션 모델 패러다임: 거대 모델(커다란 뇌) + 대규모 데이터 학습(많은 지식과 경험) 기반\n",
    "- 파운데이션 모델 기반 개발 프로세스\n",
    "\n",
    "\n",
    "![image](./img/fm5.png)\n",
    "\n",
    "> 파운데이션 모델의 특징\n",
    "- 특징 1 [대규모]: 트랜스포머 모델 + 대규모 언어 데이터 학습\n",
    "  - 테스크에 상관없이 비슷한 패턴들이 등장하고 있음\n",
    "  - 주로 비지도학습으로 훈련된 모델들'도' 많이 등장\n",
    "    <br>의미하는 바: 쉬운 데이터 수집 + 대규모 학습\n",
    "\n",
    "- 특징 2 [적응성]: 높은 파인튜닝 성능(높은 태스크 적응 성능)\n",
    "  - 믿고 쓸 수 있는 모델\n",
    "\n",
    "- 특징 3 [범용성]: 다양한 작업, 한정되지 않는 출력 지원\n",
    "  - 예시: 물체 판별\n",
    "    - 기존 20여개의 물체 종류 구분\n",
    "    - 파운데이션 모델: 만 개 이상의 물체 종류를 구분 (또는 자연어 기반의 한정되지 않은 대상에 대한 인식)\n",
    "  \n",
    "  ![image](./img/fm6.png)\n",
    "\n",
    "\n",
    "> 파운데이션 모델에 의한 AI 모델 개발의 변화\n",
    "- 과거에는 매번 모델을 새로 학습했지만, 이제는 잘 학습된 모델들을 얼마나 잘 활용하느냐가 핵심\n",
    "- 파운데이션 모델 하나 확보하는 데에 투여되는 계산 리소스는 일부 대규모 인프라 이외에 불가\n",
    "<br>\n",
    "\n",
    "- 적응 활용\n",
    "  - 활용되는 기법들: 프롬프트{엔지니어링, 튜닝}, 전이학습, 적응(Adaptation)학습, 파인튜닝\n",
    "    - Zero-shot: 처음 보는 문제를 추가 학습 없이 바로 적용(모델 자체가 가진 배경 지식 활용)\n",
    "    - Few-shot: 예제 몇 개만 보여주면 바로 적용 가능\n",
    "    - Fine-tuning: 처음부터 배우기 않아도, 조금만 알려주면 금방 적응(모델 자체를 업데이트, 모델 가중치 변경됨)\n",
    "  \n",
    "  ![image](./img/fm7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdeb276",
   "metadata": {},
   "source": [
    "### 대표적 AI 파운데이션 모델 (CLIP)\n",
    "#### AGI를 향해서\n",
    "> Human's Intelligence(cognition) = perception ∪ higher cognitive precesses\n",
    "- AI는 사람의 지능과 유사점/차이점 분석을 통해 발전\n",
    "- 2022년 11월 이전: 각 분야별로 지능의 매우 부분적 능력만을 개별적으로 모델링 시도\n",
    "- 2022년 11월(ChatGPT) 이후: 대규모 언어모델(LLM)이 높은 사고/추론 성능을 보여주기 시작 (다양한 인지 능력 벤치마크에서 인간 수준 근접)\n",
    "\n",
    "![image.png](./img/fm8.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 시각언어모델 예시 - ChatGPT with GPT-4\n",
    "\n",
    "> GPT-4(2023)\n",
    "- 자연어 입력에 국한된 기존의 거대 언어 모델에서 더 나아가 이미지, 문서, 음성 등 멀티 모달(multi-modal) 데이터를 처리할 수 있는 모델\n",
    "- GPT-4 API를 활용하여 다양한 도메인의 이미지 데이터와 결합한 모델이 개발됨(예시: 제조 AI)\n",
    "\n",
    "![image.png](./img/fm9.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 눈으로 어떤 것을 쓸까? CLIP(2021) by OpenAI\n",
    "> CLIP(2021) by OpenAI\n",
    "- AI가 언어와 시각을 통합해서 이해하는 방식을 보여준 패러다임 전환 제시\n",
    "- 파운데이션 모델로서의 특징\n",
    "  - 입력: 학습하지 않은 새로운 도메인의 입력 데이터에 대해서도 좋은 성능을 발휘(제로샷 전이)\n",
    "  - 출력: 자연어를 이용해 한 번도 본 적 없는 카테고리도 텍스트 설명만으로 출력 정의 가능(언어 인터페이스)\n",
    "\n",
    "![image.png](./img/fm10.png)\n",
    "![image.png](./img/fm11.png)\n",
    "\n",
    "> 대조 학습 기반(Contrastive Pre-training)의 언어-이미지 사전 학습\n",
    "- 인터넷 데이터를 통한 지도 학습(supervised learning)을 통해 자연어 기반 시각 개념 학습\n",
    "- 다양한 이미지-자연어 쌍으로 학습\n",
    "  - 인터넷에서 수집된 4억개의 이미지-텍스트 쌍\n",
    "  - Alt-text HTML tag, 이미지 캡션, 제목 등을 기반으로 수집\n",
    "  - 데이터 정제 과정을 거침(중복 이미지, 해상도/품질 낮은 이미지, 짧은 텍스트 등)\n",
    "  - 텍스트 인코더: Transformer\n",
    "  - 이미지 인코더: ViT-B (또는 ResNet50)\n",
    "\n",
    "![image.png](./img/fm12.png)\n",
    "![image.png](./img/fm13.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "> Remind-Transformer\n",
    "- 트랜스포머 구조 = 인코더(Encoder) + 디코더(Decoder)\n",
    "- CLIP에서는 Encoder only 구조 사용\n",
    "\n",
    "![image.png](./img/fm14.png)\n",
    "\n",
    "- 토큰이라는 단위의 입력\n",
    "- 입력된 토큰 간의 관계성을 집중하는 Attention 메커니즘으로 구성\n",
    "- L 길이의 입력 토큰은 D-차원 특징벡터(임베딩)의 배열 형태로 입력 (L x D)\n",
    "- 자연어 데이터: Sub-word 단위의 임베딩\n",
    "\n",
    "![image.png](./img/fm15.png)\n",
    "\n",
    "> Remind-Vision Transformer\n",
    "- 입력 구성\n",
    "  - 텍스트 인코더(자연어 데이터 입력): Sub-word 단위의 임베딩\n",
    "  - 이미지 인코더(이미지 데이티 입력): 패치 단위의 임베딩\n",
    "- ViT: 비전 분야에 트랜스포머를 (최소 수정으로) 적용한 모델\n",
    "\n",
    "![image.png](./img/fm16.png)\n",
    "\n",
    "- 이미지를 작은 패치(16x16x3)로 나눔\n",
    "- 각 패치를 1D로 Flatten\n",
    "- Learnable position embedding 사용\n",
    "  - 이미지 내에서 각 패치의 위치 민감 정보 추가\n",
    "  - 모델 학습 과정에서 함께 학습됨\n",
    "- Transformer encoder: 패치 처리\n",
    "- MLP Head를 통해 분류 작업 수행\n",
    "  - Head를 수정하여 다른 작업을 위한 transfer learning 활용 가능\n",
    "  - CLIP에서는 CLIP 학습법으로 학습됨\n",
    "\n",
    "![image.png](./img/fm17.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "> 대조 학습(Contrastive learning)\n",
    "- 학습 기준\n",
    "  - 목표 이미지(앵커)를 대응하는 텍스트(양성)와 가깝게\n",
    "  - 일치하지 않는 여러 텍스트(음성)와는 멀게\n",
    "\n",
    "![image.png](./img/fm18.png)\n",
    "![image.png](./img/fm19.png)\n",
    "![image.png](./img/fm20.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 1-4. CLIP 간단 응용\n",
    "> 제로샷 이미지 인식기\n",
    "- 텍스트로 원하는 물체 카테고리 리스트 준비\n",
    "- 텍스트 기반 카테고리 리스트를 텍스트 임베딩으로 변환하여 Vector DB 준비\n",
    "- 쿼리 이미지와 비교해서 가장 높은 점수의 카테고리 반환\n",
    "- 생각해보기\n",
    "  - 검색 시스템과 유사성은 무엇일까?\n",
    "  - 카테고리 이외에 어떤 것이 가능할까?\n",
    "  - 카테고리가 정말 많을 경우에 어떻게 효율화할까?\n",
    "\n",
    "![image.png](./img/fm21.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14d66e9",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #CCE5FF\"> 1차시 내용 정리</span>\n",
    "- 파운데이션 모델은 대규모 데이터로 사전학습된 범용 모델\n",
    "- 파운데이션 모델의 방대한 사전 지식을 이용해 다양한 태스크에 빠르게 적용 가능\n",
    "  - 장점: 데이터/리소스 효율적, 범용성, 확장성, 높은 성능\n",
    "- 파운데이션 모델은 대규모, 적응성, 범용성의 특징을 가짐\n",
    "- 대표 이미지 파운데이션 모델: 인터넷 상의 대규모 {텍스트, 이미지} 페어 데이터를 활용한 이미지-언어 연관성을 학습한 CLIP 파운데이션 모델과 그 제로샷 이미지 인식기 응용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3e25ba",
   "metadata": {},
   "source": [
    "## <span style=\"background-color:#E6E6FA\"> 2차시. Vision-Language Model(VLM) </span>\n",
    "> 학습 시작\n",
    "- 주어진 이미지와 장면을 분석하고 이해하는 서비스를 만들기 위해 지금까지 배운 이미지 인식기, 물체 탐지 모델로 충분할까?\n",
    "  <br>인식/탐지 테스크 출력의 제한\n",
    "- 세상에 존재하는 수많은 케이스들을 이해하고, 사용자의 요구에 맞춰 이미지를 분석할 수 있는 진짜 인텔리전트한 모델은 없을까?\n",
    "- 나 대신 어려운 그래프와 문서들을 이해하고 컨설팅 해주는 모델은 어떻게 만들지?\n",
    "- 나와 같은 것을 '보고' '대화'를 나눌 수 있는 AI 모델을 구축해보자!\n",
    "\n",
    "<br>\n",
    "\n",
    "> 학습 목표\n",
    "- 고도화된 CLIP 계열인 SigLIP의 등장 배경에 대해서 이해한다.\n",
    "- Vision-Language Model의 구조 및 구축 패턴에 대해서 파악한다.\n",
    "- LLaVA, Qwen-VL, InternVL 등 최신 시각-언어 파운데이션 모델들의 발전 동향에 대해서 설명할 수 있다.\n",
    "- (심화) 추가 학습을 요구하지 않는 CLIP계열 멀티모달 정합 모델의 심화 응용 사례를 파악한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63acf987",
   "metadata": {},
   "source": [
    "### 대표적 AI 파운데이션 모델 (CLIP)\n",
    "#### SigLIP(2023)\n",
    "\n",
    "- 기존 CLIP에서 사용한 대조학습 (Contrastive learning)의 한계는 무엇일까?\n",
    "  - 어느정도 이미 멀게 배치한 음성 데이터들에 대해서도 계속 거리를 벌리기 위해 학습이 진행됨\n",
    "- SigLIP: CLIP과 달리 일치하지 않는 음성 데이터에 제한된 영향만 받도록 손실함수 디자인을 고침\n",
    "\n",
    "![image.png](./img/fm22.png)\n",
    "\n",
    "> SigLIP은 softmax 대신 sigmoid 기반 손실함수\n",
    "\n",
    "![image.png](./img/fm23.png)\n",
    "\n",
    "> CLIP 대비 SigLIP이 압도적인 성능을 보이며 최신 VLM에 널리 활용됨(최근 SigLIP 2도 공개)\n",
    "\n",
    "![image.png](./img/fm24.png)\n",
    "\n",
    "\n",
    "#### 멀티모달 정합 응용\n",
    "\n",
    "> 멀티모달 정합(Multi-modal Alignment)\n",
    "- 서로 다른 두 가지 이상의 모달리티(예: 이미지와 텍스트) 간의 공통된 임베딩 벡터 공간을 구성하는 것\n",
    "- 서로 다른 모달리티 임베딩 간 유사도(연관성) 비교 가능\n",
    "- 대표적인 모델:\n",
    "  - CLIP(OpenAI): 이미지와 텍스트 간의 Multi-modal Slignment를 효과적으로 수행\n",
    "  - ImageBind(Meta): 더 다양한 모달리티(예: 소리, 텍스트, 이미지, 열화상, 깊이 맵을 결합)\n",
    "\n",
    "![image.png](./img/fm25.png)\n",
    "\n",
    "> ImageBIND - One Embedding Space To Bind Them All\n",
    "- 이미지, 비디오, 텍스트, 오디오, 뎁스, 열화상, IMU 모달리티 공간을 공유하도록 학습\n",
    "\n",
    "![image.png](./img/fm26.png)\n",
    "\n",
    "> VLM의 눈으로 응용\n",
    "- CLIP 기반 VLM들 리스트\n",
    "  - BLIP-2: CLIP + OPT/Flan T5 결합\n",
    "  - InstructBLIP: BLIP-2의 instruction tuning 버전\n",
    "  - LLaVA: CLIP + Vicuna\n",
    "  - MiniGPT-4: CLIP + Vicuna 기반\n",
    "  - mPLUG-Owl: CLIP 기반 Alibaba의 VLM\n",
    "- SigLIP 기반 VLM들 리스트\n",
    "- PaLI-X: SigLIP + PaLM 결합\n",
    "- SmolVLM\n",
    "<br>\n",
    "→ 최근에는 CLIP, SigLIP의 성공적인 레시피를 기반으로 특화 Vision encoder 모델들이 개발되는 추세"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f75811",
   "metadata": {},
   "source": [
    "### Vision-Language Models\n",
    "#### 3-1. AGI를 향해서\n",
    "\n",
    "> Human's Intelligence(cognition) = perception ∪ higher cognitive precesses\n",
    "\n",
    "![image](./img/fm27.png)\n",
    "\n",
    "#### 3-1. 멀티모달 언어 모델\n",
    "\n",
    "> 이미지, 소리, 비디오 등 다양한 모달리티를 함께 이해하고 처리할 수 있는 언어 모델\n",
    "- 대표적인 모델\n",
    "  - ChatGPT< Claude, LLaVA(2023), InstructBLIP, Qwen-VL, LLaMA-Vision, smolVLM, Phi, HyperClovaX-SEED-Vision, etc\n",
    "- 응용 사례\n",
    "  - 텍스트와 이미지를 결합한 대화형 AI, 이미지 설명, 문서 이해, 비디오 분석 등 다양한 분야에서 사용\n",
    "\n",
    "![image](./img/fm28.png)\n",
    "\n",
    "#### 3-2. LLaVA(Large Language and Vision Assistant; 2023)\n",
    "> Vision과 Language 모델을 결합한 모델(VLM)로, 텍스트와 이미지를 동시에 이해\n",
    "- 주요 특징\n",
    "  - 이미지 인식과 텍스트 생성을 결합하여, 이미지 설명 생성 또는 시각적 질문 응답 작업에서 뛰어난 성능\n",
    "  - 이미지, 명령(Instruction), 답변이 주어진 데이터셋을 구축하여 Instruction tuning으로 학습\n",
    "- 응용 사례\n",
    "  - 이미지 기반 질문 응답(Vision QA), 이미지 설명 생성, 시각적 정보 기반 대화 등\n",
    "\n",
    "<br>\n",
    "\n",
    "> LLaVA 모델 특징\n",
    "- 효율적인 메모리 사용: 적은 자원으로 큰 모델을 효과적으로 학습\n",
    "- 다중 모달 학습: 텍스트와 시각 데이터를 결합하여 응답을 생성\n",
    "- Fine-tuning: 특정 잡업에 맞춰 모델을 미세조정하여 사용\n",
    "\n",
    "![image](./img/fm29.png)\n",
    "\n",
    "> Step 1: 사전학습(Pre-training)\n",
    "- 표현 공유\n",
    "  - 이미지를 텍스트 표현을 변환하는 선형 레이어(Projection layer)를 학습하여 텍스트와 이미지를 공통된 토큰 표현으로 처리\n",
    "  - 전체 모델을 다시 훈련하지 않으므로 자원과 시간 절감\n",
    "- 효율적인 학습\n",
    "  - 적은 파라미터만 (Projection layer) 학습\n",
    "\n",
    "![image](./img/fm30.png)\n",
    "\n",
    "> Step 2: Fine-tuning\n",
    "- 표현 공유\n",
    "  - 특정 작업에 맞춰 [선형 레이어]와 [언어 모델] 등 필요한 부분만 미세 조정으로 강화\n",
    "- 효율적인 학습\n",
    "  - FP16과 같은 정밀도 최적화를 통해 적은 메모리로 큰 모델 학습 가능\n",
    "  - 저비용 학습 기법을 통해 메모리 사용량 절감\n",
    "\n",
    "![image](./img/fm31.png)\n",
    "\n",
    "> LLaVA 학습 데이터(합성 데이터)\n",
    "- ChatGPT를 활용하여 시각 설명 데이터(visual instruction data) 생성\n",
    "  - 기존 COCO 데이터셋: 이미지와 대응하는 이미지 설명, 물체 좌표(bounding box)를 라벨 데이터로 제공\n",
    "  - coco 데이터셋의 이미지와 라벨 데이터들을 가지고, ChatGPT를 이용하여 자동으로 역 질의 생성\n",
    "  - 질의 타입: 대화(conversion), 자세한 설명(detailed description), 그리고 복잡한 추론(complex reasoning)\n",
    "\n",
    "<br>\n",
    "\n",
    "#### 3-2. LLaVa(Large Language and Vision Assistant; 2023)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
